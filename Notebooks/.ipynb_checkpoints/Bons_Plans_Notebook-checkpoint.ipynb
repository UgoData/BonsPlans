{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd, json, requests, numpy as np\n",
    "#from dataiku import Folder\n",
    "import re\n",
    "from time import time\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn import cluster\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "## Nearest neighbours\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "## Cosine distance from SciPy\n",
    "from scipy.spatial.distance import cosine\n",
    "from pandas.io.json import json_normalize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "import unicodedata\n",
    "import pickle\n",
    "import mysql.connector\n",
    "from Normalizer import *\n",
    "from EnrichProduct import *\n",
    "import MySQLdb\n",
    "import pandas.io.sql as psql\n",
    "import sys\n",
    "from math import fabs\n",
    "import apriori\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models_path='Models/multiNB.p'\n",
    "model = pickle.load(open(models_path, \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dictionnaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_cat = ['1001','1002','1003','1004','1005','1006','1007','1008','1010','1011',\n",
    "            '2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ponderation recuperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Dictionnaire de ponderation des modeles\n",
    "def getDictPond():\n",
    "    conn= mysql.connector.connect(host='localhost',database='dataiku',user='dkuadmin',password='Dataiku!')\n",
    "    req = \"SELECT * from pondbonsplans order by date desc limit 1\"\n",
    "    pond_df = pd.read_sql(req, conn)\n",
    "    pond_df_temp=pond_df.to_dict(orient='list')\n",
    "    return {k:v[0] for (k,v) in pond_df_temp.iteritems()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Bons Plans informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mapFeatureDF(features_df):\n",
    "    \"\"\" map features to user_info \"\"\"\n",
    "    return features_df['features']['listeBonsPlans'],features_df['features']['listeMagasins'],features_df['features']['listeMarchands']\n",
    "\n",
    "def mapMarchands(rawJsonBP):\n",
    "    \"\"\" Mapping des donnees sur les marchands \"\"\"\n",
    "    marchands=pd.DataFrame(columns=['marchId','magMarch'])\n",
    "\n",
    "    count=0\n",
    "    for i in range(len(rawJsonBP)):\n",
    "        sub_bp=rawJsonBP[1][i]\n",
    "        marchands.loc[i]=[sub_bp['id'],sub_bp['nom'].encode('utf-8')]\n",
    "\n",
    "        count+=1\n",
    "    return marchands\n",
    "\n",
    "def mapMagasins(rawJsonBP):\n",
    "    \"\"\" Mapping des donnees sur les magasins \"\"\"\n",
    "\n",
    "    # Set empty dataframe with all possible columns\n",
    "    df_empty_allcol=pd.DataFrame(columns=['adresse.code-postal', 'adresse.commune', 'adresse.lat-lng', 'adresse.rue', 'categories', 'horaires', 'id', 'image', 'marchand', 'nom', 'rmw', 'siteweb'])\n",
    "    # Load data into dataframe\n",
    "    df_raw=json_normalize(rawJsonBP[1])\n",
    "    # Merge to be sure having all the columns\n",
    "    result=df_empty_allcol.append(df_raw, ignore_index=True)\n",
    "    # Keep only interesting columns\n",
    "    result=result[['id','adresse.code-postal', 'adresse.commune', 'adresse.lat-lng', 'adresse.rue', 'categories', 'horaires','marchand', 'nom', 'rmw']]\n",
    "    magasins=pd.DataFrame(result.values,columns=['magId','magZipcode','magDepcom',\n",
    "                                    'magLatLong','magRue',\n",
    "                                    'magCat','magHor',\n",
    "                                    'magMarch','magNom','magRmw'])\n",
    "    # Fill na\n",
    "    magasins.fillna('',inplace=True)\n",
    "    # Encoding\n",
    "    magasins.magNom=magasins.magNom.str.encode('utf-8')\n",
    "    magasins.magRue=magasins.magRue.str.encode('utf-8')\n",
    "    magasins.magDepcom=magasins.magDepcom.str.encode('utf-8')\n",
    "    return magasins\n",
    "\n",
    "def mapBonsPlans(rawJsonBP):\n",
    "    \"\"\" Mapping des donnees sur les bons plans \"\"\"\n",
    "    time_bp=time()\n",
    "\n",
    "    # Set empty dataframe with all possible columns\n",
    "    df_empty_allcol=pd.DataFrame(columns=['conditions', 'date-debut', 'date-fin', 'description', 'distance', 'id', 'image', 'libelle', 'magasin', 'metadata.prix unitaire', 'metadata.source', 'metadata.unité', 'offre.categorie', 'offre.description', 'offre.id', 'offre.image', 'offre.libelle', 'offre.magasin', 'offre.marque', 'offre.metadata.source', 'offre.prix', 'prix'])\n",
    "    # Load data into dataframe\n",
    "    df_raw=json_normalize(rawJsonBP[0])\n",
    "    # Merge to be sure having all the columns\n",
    "    result=df_empty_allcol.append(df_raw, ignore_index=True)\n",
    "    # Keep only interesting columns\n",
    "    result=result[['conditions', 'date-debut', 'date-fin', 'description', 'distance', 'id','libelle', 'magasin','offre.categorie', 'offre.description', 'offre.id','offre.libelle', 'offre.magasin', 'offre.marque','offre.prix', 'prix','metadata.source', 'metadata.prix unitaire', 'metadata.unité']]\n",
    "    bons_plans=pd.DataFrame(result.values,columns=['conditions','dateDebut','dateFin','description','distance',\n",
    "                                     'id','libelle',\n",
    "                                     'magId',\n",
    "                                     'offreCat','offreDesc',\n",
    "                                     'offreId','offreLib','offreMag','offreMarq',\n",
    "                                     'offrePrix','prix','source','prix_unitOff', 'unitOff'])\n",
    "    # Fill na\n",
    "    bons_plans.fillna('',inplace=True)\n",
    "    # Encoding\n",
    "    bons_plans.libelle=bons_plans.libelle.str.encode('utf-8')\n",
    "    bons_plans.offreLib=bons_plans.offreLib.str.encode('utf-8')\n",
    "    bons_plans.offreMag=bons_plans.offreMag.str.encode('utf-8')\n",
    "    bons_plans.offreMarq=bons_plans.offreMarq.str.encode('utf-8')\n",
    "\n",
    "    print 'complet : ',result.shape[0]\n",
    "    print 'MAP BP :', time()-time_bp\n",
    "    return bons_plans\n",
    "\n",
    "def joinBPMag(rawJsonBP):\n",
    "\n",
    "    try :\n",
    "        result=pd.merge(mapBonsPlans(rawJsonBP), mapMagasins(rawJsonBP), on='magId',how='left')\n",
    "    except:\n",
    "        result=pd.DataFrame(columns=['nothing'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get user informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def locUser(features_df):\n",
    "        return features_df['features']['localisationUser']\n",
    "        \n",
    "def getUserId(features_df):\n",
    "    return features_df['features']['idUser']\n",
    "\n",
    "def modeUser(user_info):\n",
    "    return ['walking'] #'bicycling','walking','transit','driving'\n",
    "\n",
    "def getUsersMatrix():\n",
    "    conn= mysql.connector.connect(host='localhost',database='dataiku',user='dkuadmin',password='Dataiku!')\n",
    "    req = \"SELECT * from userinformations\"\n",
    "    couponscores = pd.read_sql(req, conn)\n",
    "    return couponscores\n",
    "\n",
    "def getUserMatrix(df,user_id):\n",
    "    return df[df['user_id']==user_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get product categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Get pickle and fit new data ####\n",
    "\n",
    "def getCategory( df_topredict):\n",
    "    \"\"\"Get Category by modelling\n",
    "    Return : prediction in columns category order\"\"\"\n",
    "\n",
    "    # Prepare dataset to predict\n",
    "    df_topredict=fromDFToInput(df_topredict)\n",
    "\n",
    "    # Predict result\n",
    "    predict_time=time()\n",
    "    result_raw=model.predict(df_topredict)\n",
    "    print 'PREDICT PICKLE', time()-predict_time\n",
    "    \n",
    "    result = pipeline_output_formatting(result_raw, list_cat)\n",
    "\n",
    "    # Cleaning df to predict\n",
    "    del df_topredict['NomDescr']\n",
    "    del df_topredict['nomProduit']\n",
    "    del df_topredict['descriptionProduit']\n",
    "    df_topredict['analytics_category']=result\n",
    "    return df_topredict\n",
    "\n",
    "### Formatting the output of the pipeline\n",
    "def pipeline_output_formatting(result_raw, list_cat):\n",
    "    result = []\n",
    "    for x in result_raw:\n",
    "        temp = []\n",
    "        for idy, y in enumerate(x):\n",
    "            if y>0:\n",
    "                temp.append(list_cat[idy])\n",
    "        result.append(temp)\n",
    "    return result\n",
    "    \n",
    "### Mapping df to predict into good format ###    \n",
    "\n",
    "def fromDFToInput(df):\n",
    "    time_fromDFTO=time()\n",
    "    df.fillna('', inplace=True)\n",
    "\n",
    "    df['offreDesc']=df['offreDesc'].map(lambda x:x.encode('utf-8') if x<>'' else x)\n",
    "\n",
    "    time_norm=time()\n",
    "    list_stop_word_french=['alors','au','aucuns','aussi','autre','avant','avec','avoir','bon','car','ce','cela','ces','ceux','chaque',\n",
    "                   'ci','comme','comment','dans','des','du','dedans','dehors','depuis','devrait','doit','donc','dos','début',\n",
    "                   'elle','elles','en','encore','essai','est','et','eu','fait','faites','fois','font','hors','ici','il','ils',\n",
    "                   'je','juste','la','le','les','leur','là','ma','maintenant','mais','mes','mine','moins','mon','mot','même',\n",
    "                   'ni','nommés','notre','nous','ou','où','par','parce','pas','peut','peu','plupart','pour','pourquoi','quand',\n",
    "                   'que','quel','quelle','quelles','quels','qui','sa','sans','ses','seulement','si','sien','son','sont','sous',\n",
    "                   'soyez','sujet','sur','ta','tandis','tellement','tels','tes','ton','tous','tout','trop','très','tu','voient',\n",
    "                   'vont','votre','vous','vu','ça','étaient','état','étions','été','être']\n",
    "\n",
    "\n",
    "    # Suppress number\n",
    "    reg_numb = re.compile('[^\\D]')\n",
    "    # Suppress punctuation\n",
    "    reg_ponct = re.compile('[^a-z 0-9ÀÁÂÃÄÅàáâãäåÒÓÔÕÖØòóôõöøÈÉÊËèéêëÇçÌÍÎÏìíîïÙÚÛÜùúûüÿÑñ²°Ø×ßŠ”�œ…]')\n",
    "\n",
    "\n",
    "    # Suppress stop words\n",
    "    french_stopwords_ini=stopwords.words('french')\n",
    "    french_stopwords_ini.extend(list_stop_word_french)\n",
    "    french_stopwords = set(french_stopwords_ini)\n",
    "\n",
    "\n",
    "    # Stemming of words\n",
    "    stemmer = FrenchStemmer()\n",
    "\n",
    "\n",
    "    df['nomProduit']=df['offreLib'].map(lambda x:normaliz(x,french_stopwords,reg_numb,reg_ponct,stemmer))\n",
    "    df['descriptionProduit']=df['offreDesc'].map(lambda x:normaliz(x,french_stopwords,reg_numb,reg_ponct,stemmer))\n",
    "    print 'Time Normalize :', time()-time_norm\n",
    "    df['prix_old']=df['prix'].map(lambda x: float(x) if x<>'' else 0.0)\n",
    "    df['prix']=df['offrePrix'].map(lambda x: float(x) if x<>'' else 0.0)\n",
    "\n",
    "\n",
    "    df['NomDescr']=df['nomProduit']+' '+df['descriptionProduit']\n",
    "    print 'TIME FROM DF TO INPUT :', time()-time_fromDFTO\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich product informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Recuperation des temps de trajet ###\n",
    "def setTpsTrajet(product_info,latlong_origin,modes):\n",
    "    \"\"\"Recuperation du temps de trajet et mise en forme\"\"\"\n",
    "    merg=getTpsTrajet(product_info,latlong_origin,modes)\n",
    "    product_info['travelTime']=merg['walking'].map(lambda x:timeTransformation(x))\n",
    "    product_info['travelTimeRaw']=merg['walking']\n",
    "    return product_info\n",
    "\n",
    "### Modulation du prix ###\n",
    "def setPrice(df):\n",
    "    result=df['prix']\n",
    "    if df['prix_old']>0:\n",
    "        result=df['prix_old']\n",
    "    return result\n",
    "\n",
    "\n",
    "### Add columns to result for transposition into matrix_product ###\n",
    "def enrichDataProduct(bon_plans, locUser, modeUser, user_info):\n",
    "    # Add column analytics category\n",
    "    t0_cat=time()\n",
    "    #bon_plans['analytics_category']=bon_plans.apply(getCategoryBpTemp,axis=1)\n",
    "    bon_plans=getCategory(bon_plans)\n",
    "    print 'TIME_FOR_CAT',time()-t0_cat\n",
    "    #print 'Category distinct :', bon_plans['analytics_category'].unique()\n",
    "    # Add travel time\n",
    "    time_tps_trajet=time()\n",
    "    bon_plans=setTpsTrajet(bon_plans,locUser,modeUser)\n",
    "    print 'TimeTempsTrajet :', time()-time_tps_trajet\n",
    "    #print 'Temps trajet unique :', bon_plans['travelTime'].unique()\n",
    "\n",
    "    # Caculate new columns of quantity and unity\n",
    "    bon_plans['unite']=bon_plans.offreDesc.map(lambda x:getUnit(x).replace(' ',''))\n",
    "    #print 'Unite :', bon_plans['unite'].unique()\n",
    "    bon_plans['quantite']=bon_plans.offreDesc.map(lambda x:getQuantity(x))\n",
    "    #print 'quantite :', bon_plans['quantite'].unique()\n",
    "    bon_plans['unite_val']=bon_plans.unite.map(lambda x:dictUnit(x))\n",
    "    #print 'unite_val :', bon_plans['unite_val'].unique()\n",
    "    bon_plans['quantite_unite']=bon_plans['unite_val']*bon_plans['quantite']\n",
    "    #print 'quantite_unite :', bon_plans['quantite_unite'].unique()\n",
    "    bon_plans['price_unit']=bon_plans.apply(priceUnit,axis=1)\n",
    "    #print 'price_unit :', bon_plans['price_unit'].unique()\n",
    "    del bon_plans['unite_val']\n",
    "\n",
    "    # Quantile\n",
    "    #bon_plans['quantile_prixraw']=bon_plans.groupby(['requete'])['prix'].apply(lambda x: qcut_custom(x,4))\n",
    "    bon_plans['quantile_prixquantite']=pct_rank_qcut(bon_plans['price_unit'],2)\n",
    "    #print 'quantile_prixquantite :', bon_plans['quantile_prixquantite'].unique()\n",
    "\n",
    "    # Reduction type\n",
    "    bon_plans['reduc_type']=bon_plans['libelle'].map(lambda x:getReducType(x))\n",
    "    #print 'reduc_type :', bon_plans['reduc_type'].unique()\n",
    "\n",
    "    # Bio\n",
    "    #print bio(bon_plans.ix[0])\n",
    "    bon_plans['bio']=bon_plans.apply(bio,axis=1)\n",
    "\n",
    "    # Info complementaire sur les bons plans\n",
    "    bon_plans['prix_old']=bon_plans.apply(setPrice,axis=1)\n",
    "    bon_plans['pctReduc']=bon_plans.apply(pctReduc, axis=1)\n",
    "    #print 'pctReduc :', bon_plans['pctReduc'].unique()\n",
    "    bon_plans['mntReduc']=bon_plans.apply(mntReduc, axis=1)\n",
    "    #print 'mntReduc :', bon_plans['mntReduc'].unique()\n",
    "\n",
    "    # Favorite categories by user\n",
    "    regex = re.compile('[\\[\\] ]')\n",
    "    bon_plans['analytics_userCategory']=bon_plans.apply(getAnalyticsUserCategory,args=(user_info,regex,),axis=1)\n",
    "    #print 'analytics_userCategory :', bon_plans['analytics_userCategory'].unique()\n",
    "\n",
    "    # Sensitivity to reduction\n",
    "    bon_plans['analytics_reduc']=bon_plans.apply(reduc_scoring,args=(user_info,regex,),axis=1)\n",
    "    #print 'analytics_reduc :', bon_plans['analytics_reduc'].unique()\n",
    "\n",
    "    return bon_plans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of empty product matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createMatrixProduct(bon_plans):\n",
    "    time_mat=time()\n",
    "\n",
    "    # Creation of the result matrix\n",
    "    matrix_product=pd.DataFrame()\n",
    "    matrix_product['id']=bon_plans['id']\n",
    "    matrix_product['offreLib']=bon_plans['offreLib']\n",
    "    matrix_product['offreDesc']=bon_plans['offreDesc']\n",
    "\n",
    "\n",
    "    # Univers de consommation\n",
    "    regex = re.compile('[\\[\\] ]')\n",
    "    bon_plans['analytics_category_list']=bon_plans['analytics_category']\n",
    "    #print 'LISTANALYTICS', type(bon_plans['analytics_category_list'][0])\n",
    "\n",
    "    # Quantile  add columns prixSens_cat_lowPrice or highPrice\n",
    "    # Sensitivity to reduction  add columns reductionSens_cat_typereduc\n",
    "    # Liste des categories du produit pour toutes les avoir\n",
    "    listCatProd=['1000','1001','1002','1003','1004','1005','1006','1007','1008','1009','1010','1011',\n",
    "                 '2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012']\n",
    "    time_list_cat=time()\n",
    "    for i in listCatProd:\n",
    "        # Univers Conso\n",
    "        matrix_product['universConso_'+str(i)]=bon_plans['analytics_category_list'].map(lambda x:1 if i in x else 0)\n",
    "        # Price sensitivity\n",
    "        matrix_product['prixSens_'+str(i)+'_lowPrice']=bon_plans['quantile_prixquantite'].map(lambda x:1 if x==1 else 0)*matrix_product['universConso_'+str(i)]\n",
    "        matrix_product['prixSens_'+str(i)+'_highPrice']=bon_plans['quantile_prixquantite'].map(lambda x:1 if x==2 else 0)*matrix_product['universConso_'+str(i)]\n",
    "        # Reduction Sensitivity\n",
    "        list_red=['carte','noreduc','2iemGrat','3iemGrat','freeImm','autres']\n",
    "        for y in list_red:\n",
    "            matrix_product['reductionSens_'+str(i)+'_'+y]=bon_plans['reduc_type'].map(lambda x:1 if x==y else 0)*matrix_product['universConso_'+str(i)]\n",
    "\n",
    "        # Bio Sensitiv\n",
    "        matrix_product['bioSens_'+str(i)]=bon_plans['bio']*matrix_product['universConso_'+str(i)]\n",
    "    print 'TIME LOOP CREATE :', time()-time_list_cat\n",
    "    # Magasin\n",
    "    matrix_product['mag_Fnac']=bon_plans['magMarch'].map(lambda x:1 if 'fnac' in x.lower() else 0)\n",
    "    matrix_product['mag_Carrefour']=bon_plans['magMarch'].map(lambda x:1 if 'carrefour' in x.lower() else 0)\n",
    "    matrix_product['mag_Monoprix']=bon_plans['magMarch'].map(lambda x:1 if 'monop' in x.lower() else 0)\n",
    "    matrix_product['mag_Autres']=bon_plans['magMarch'].map(lambda x:1 if 'other' in x.lower() else 0)\n",
    "\n",
    "    # Quantite TO BE MODIFIED\n",
    "    matrix_product['quantity_unit']=0\n",
    "    matrix_product['quantity_family']=0\n",
    "\n",
    "    # Info complementaire sur les bons plans\n",
    "    matrix_product['pctReduc']=bon_plans['pctReduc']\n",
    "    matrix_product['mntReduc']=bon_plans['mntReduc']\n",
    "\n",
    "    # Travel time\n",
    "    matrix_product['travelTime']=bon_plans['travelTime']\n",
    "\n",
    "\n",
    "    print 'Time matrix_product :', time()-time_mat\n",
    "    return matrix_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating similarities with past buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getHistoricBuy():\n",
    "    \"\"\" Get all the past buyings of the user. Put them into a database\"\"\"\n",
    "    # Connecting to DB\n",
    "    conn= mysql.connector.connect(host='localhost',database='dataiku',user='dkuadmin',password='Dataiku!')\n",
    "\n",
    "    req='select * from dataiku.userbonsplans where (clipped=True or burned=True)'\n",
    "    df_buy = pd.read_sql(req, conn)\n",
    "\n",
    "    return df_buy\n",
    "\n",
    "def getHistoryBuyFromUser(df,features_df):\n",
    "    return df[df['userId']==features_df['features']['idUser']]\n",
    "\n",
    "def asHist(old_df):\n",
    "    \"\"\" Return True if the user has history\"\"\"\n",
    "    result=False\n",
    "    if old_df.shape[0]>0:\n",
    "        result=True\n",
    "    return result\n",
    "\n",
    "def mergeNewOldDB(bon_plans,features_df,old_plans):\n",
    "    \"\"\" Merge the new bons plans and the ones bought in the pass by the user\"\"\"\n",
    "    # Get weight\n",
    "    old_plans.loc[:,'weight']=old_plans.apply(getWeights,axis=1)\n",
    "    #print old_plans.loc[:,'weight']\n",
    "\n",
    "    op=old_plans[['id','offreLib','offreDesc','prix','source','weight']]\n",
    "    op.loc[:,'from']='old'\n",
    "    # Get new bons plans\n",
    "    bon_plans['weight']=0\n",
    "    np=bon_plans[['id','offreLib','offreDesc','prix','source','weight']]\n",
    "    np.loc[:,'from']='new'\n",
    "    # Merge data\n",
    "    return pd.concat([op,np])\n",
    "\n",
    "def similarityCalculation(df):\n",
    "    \"\"\"Calculate the similarity between products\"\"\"\n",
    "    # Creating common index\n",
    "    df['id2']=range(df.shape[0])\n",
    "    df.set_index('id2',inplace=True)\n",
    "\n",
    "    # Create a full description columns\n",
    "    df.loc[:,'fulldescriptionProduit']=pd.Series(df.loc[:,'offreLib'].str.cat(df.loc[:,'offreDesc'].values, sep=' '),index=df.index)\n",
    "\n",
    "\n",
    "    # Scale the price with MinMaxScaler : price is in [0,1] range (for the distance)\n",
    "    # Price is very low for the majority of data => log to recenter the distribution\n",
    "    scaler = MinMaxScaler()\n",
    "    df['prix']=df['prix'].map(lambda x:x if x>0 else 0.00001)\n",
    "    df.loc[:,'scaledlogprix']=scaler.fit_transform(np.log(df.loc[:,'prix'].values).reshape(-1, 1))\n",
    "    #print df\n",
    "\n",
    "    # TFIDF\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=0.001,max_df=0.8,ngram_range=(1,2))\n",
    "    tfidf_productDescr=tfidf_vectorizer.fit_transform(df['fulldescriptionProduit'])\n",
    "\n",
    "    ## Concatenate index / TFIDF product name / Product price - Dense matrix\n",
    "    dist_data_dense=np.hstack((df.index.values.reshape(-1,1),tfidf_productDescr.toarray(),df.loc[:,'scaledlogprix'].values.reshape(-1,1)))\n",
    "\n",
    "    def custom_distance(x, y):\n",
    "        \"\"\" Creating the custom distance function between records\"\"\"\n",
    "        i, j = int(x[0]), int(y[0])  # extract index which is id from pandas df\n",
    "        # Computes cosine similarity on tf idf features (all features execpt id (0) and price (-1))\n",
    "        tfidf_dist=cosine(dist_data_dense[i,1:-1],dist_data_dense[j,1:-1])\n",
    "        # Price distance - absolute values\n",
    "        price_distance=np.abs(dist_data_dense[i,-1]-dist_data_dense[j,-1])\n",
    "        return tfidf_dist+price_distance\n",
    "\n",
    "    ## Distinction between old and new products\n",
    "    nb_old=df[df['from']=='old'].shape[0]\n",
    "    nb_new=df.shape[0]-nb_old\n",
    "    print 'NB_OLD and NB_NEW:',nb_old, nb_new\n",
    "\n",
    "\n",
    "    ## Distance pairwise\n",
    "    dist_df=pd.DataFrame(dist_data_dense)\n",
    "    return pd.DataFrame(pairwise_distances(dist_df.iloc[0:nb_old],dist_df.iloc[nb_old:],metric=custom_distance).transpose(),columns=['Row'+str(i) for i in range(1, nb_old+1)],index=df.id[nb_old:])\n",
    "\n",
    "def convertNeighborUnit(df):\n",
    "    \"\"\" Convert the 0: close to 2:far unit into a 0:far to 1:close unit\"\"\"\n",
    "    for i in df.columns:\n",
    "        #for y in range(len(df[i])):\n",
    "            #df.ix[y,i]=fabs(df.ix[y,i]-2)/2\n",
    "        df[i]=df[i].apply(lambda x:float(x)/2)\n",
    "    return df\n",
    "\n",
    "def getWeights(df):\n",
    "    result=0\n",
    "    if df['burned']:\n",
    "        result=1\n",
    "    elif df['clipped']:\n",
    "        result=.5\n",
    "    return result\n",
    "\n",
    "def getHistoricalWeight(df_with_weight, df_with_neight):\n",
    "    # Get weight clipped or burned\n",
    "    df_weight=df_with_weight.ix[df_with_weight['weight']>0,'weight']\n",
    "    #print df_weight.shape\n",
    "    # multiplication weight with neightbor\n",
    "    df_res=pd.DataFrame(df_with_neight.values*df_weight.transpose().values, columns=df_with_neight.columns, index=df_with_neight.index)\n",
    "    df_res['hist_user_dist']=df_res.sum(axis=1)/df_res.shape[1]\n",
    "\n",
    "    return df_res\n",
    "\n",
    "def mergeScoringToBonsPlans(bons_plans,df_score):\n",
    "    \"\"\" Merge the scoring values to the bons plans \"\"\"\n",
    "    #print bons_plans.columns\n",
    "    #print df_score.columns\n",
    "    df_score2=df_score[['hist_user_dist']]\n",
    "    df_score2['hist_user_dist'][df_score2['hist_user_dist']==0]=max(df_score2['hist_user_dist'])\n",
    "    return pd.merge(bons_plans, df_score2, left_on='id',right_on=df_score2.index.values,how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Comment : all the calculation of frequencies might be done in batch and saved into a proper database\n",
    "def loadHistoricalData(self):\n",
    "    # Connecting to DB\n",
    "    conn= mysql.connector.connect(host='localhost',database='dataiku',user='dkuadmin',password='Dataiku!')\n",
    "\n",
    "    req='select userId,id as prodId, sum(case when burned=1 then 1 when clipped=1 then .5 else 0 end) rating \\\n",
    "        FROM userbonsplans \\\n",
    "        where (case when burned=1 then 1 when clipped=1 then .5 else 0 end)>0 \\\n",
    "        group by userId,id'\n",
    "    df_hist = pd.read_sql(req, conn)\n",
    "\n",
    "    return df_hist\n",
    "\n",
    "def formatData(df,user_id):\n",
    "    # Create a full dataframe with user in row and product in columns\n",
    "    cross_data=pd.crosstab(df['userId'],df['prodId'],values=df['rating'],aggfunc=sum)\n",
    "    cross_data=cross_data.fillna(0)\n",
    "    #print cross_data\n",
    "    # Convert this dataframe into a input for apriori method : list of unique items combinaison\n",
    "    dataset_full=[]\n",
    "    for i in range(cross_data.shape[0]):\n",
    "        sub_data=[]\n",
    "        for y in range(cross_data.shape[1]):\n",
    "            if cross_data.iloc[i,y]>0:\n",
    "                sub_data.append(cross_data.columns[y])\n",
    "        dataset_full.append(sub_data)\n",
    "    #print 'Input for the APRIORI method :', dataset_full\n",
    "    # Line of the concerned user : the historic data must not be impacted by the user historic\n",
    "    line_user=cross_data.index.get_loc(user_id)\n",
    "    # Extract user from historic\n",
    "    dataset_user=[dataset_full[i] for i in [line_user]]\n",
    "    # Extract all non user historic\n",
    "    dataset=[dataset_full[i] for i in (range(len(dataset_full))) if i not in [line_user]]\n",
    "\n",
    "    return dataset, dataset_user\n",
    "\n",
    "def aprioriMethod(dataset,confidence):\n",
    "    L,support_data=apriori.apriori(dataset)\n",
    "    #print 'L_s :', L, support_data, dataset\n",
    "    # Prune with confidence\n",
    "    rules=apriori.generateRules(L,support_data,min_confidence=confidence)\n",
    "    return pd.DataFrame(list(rules),columns=['buy_ini','buy_suggest','confidence'])\n",
    "\n",
    "def aprioriSelection(df_rule,dataset_user):\n",
    "    #print 'DF_RULES :', df_rule\n",
    "    frozenSet_User=apriori.createC1(dataset_user)\n",
    "    #print 'FROZEN USER :', frozenSet_User\n",
    "    result=pd.DataFrame(columns=['buy_ini','buy_suggest','confidence'])\n",
    "    #print 'TEST :', itertools.combinations(frozenSet_User, 2)\n",
    "\n",
    "    for su in itertools.combinations(frozenSet_User, 2):\n",
    "        #print 'INSIDE LOOP :', su\n",
    "        result=result.append(pd.DataFrame(df_rule[(df_rule['buy_ini']==su[1])&(df_rule['buy_suggest']!=su[0])]),ignore_index=True)\n",
    "        result=result.append(pd.DataFrame(df_rule[(df_rule['buy_ini']==su[0])&(df_rule['buy_suggest']!=su[1])]),ignore_index=True)\n",
    "    for su in itertools.combinations(frozenSet_User, 1):\n",
    "        result=result.append(pd.DataFrame(df_rule[(df_rule['buy_ini']==su[0])]),ignore_index=True)\n",
    "    result.drop_duplicates(inplace=True)\n",
    "    #print 'Result in apriori selection :', result\n",
    "    return result.apply(convertFrozenToList,axis=1)\n",
    "\n",
    "def convertFrozenToList(row):\n",
    "    return list(row['buy_suggest']), row['confidence']\n",
    "\n",
    "def aprioriResults(df):\n",
    "    dic={}\n",
    "    #print 'df in apriori results :', df\n",
    "    for i in df:\n",
    "        for y in i[0]:\n",
    "            # si id already exists in dict\n",
    "            if y in dic:\n",
    "                dic[y]+=i[1]\n",
    "            # create new key\n",
    "            else:\n",
    "                dic[y]=i[1]\n",
    "    #print 'PRINT DICT :', dic\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster user formulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clusterKMeans(matrix_users,k_numb):\n",
    "    \"\"\" Update user information by adding a cluster name \"\"\"\n",
    "    # Modify matrix user\n",
    "    matrix_users.set_index(('user_id'),inplace=True)\n",
    "    matrix_users=pd.get_dummies(matrix_users,prefix_sep='_', columns=['age','sexe','situation','zone'], sparse=False)\n",
    "    # KMeans\n",
    "    k_means = cluster.KMeans(n_clusters=k_numb)\n",
    "    cluster_labels=k_means.fit_predict(matrix_users)\n",
    "    #silhouette_avg = silhouette_score(input_data, cluster_labels)\n",
    "    matrix_users['cluster']=cluster_labels\n",
    "    return matrix_users\n",
    "\n",
    "# Not used for the moment\n",
    "def bestKChoice(nb_ligne, input_data, silhouette_avg_old):\n",
    "    \"\"\" Method to find the best k based on the silhouette \"\"\"\n",
    "    silhouette_avg=silhouette_avg_old\n",
    "    bestK=2\n",
    "    for k in [2,3,4,5,6]:\n",
    "        if nb_ligne>=k:\n",
    "            # The silhouette_score gives the average value for all the samples.\n",
    "            # This gives a perspective into the density and separation of the formed\n",
    "            # clusters\n",
    "            k_means = cluster.KMeans(n_clusters=k)\n",
    "            cluster_labels=k_means.fit_predict(input_data)\n",
    "            silhouette_avg = silhouette_score(input_data, cluster_labels)\n",
    "            print(\"For n_clusters =\", n_clusters,\n",
    "                  \"The average silhouette_score is :\", silhouette_avg)\n",
    "            #The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. \n",
    "            #Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.\n",
    "            if silhouette_avg>silhouette_avg_old:\n",
    "                bestK=k\n",
    "    return bestK\n",
    "\n",
    "def findBestProductByCluster(df_hist,user_id,matrix_users,bons_plans):\n",
    "    \"\"\" Return the historical matrix with a frequency by cluster column \"\"\"\n",
    "    # Find all the users from the same cluster than the user but not the user ...\n",
    "    userFromSameCluster=matrix_users[matrix_users['cluster']==matrix_users.loc[user_id,'cluster']].index\n",
    "    # Find product frequently bought by the cluster\n",
    "    prod_clust=df_hist[(df_hist['userId'].isin(userFromSameCluster))&(df_hist['userId']!=user_id)]\n",
    "    # Number of distinct users\n",
    "    distinct_user=prod_clust.groupby(['userId'])['id'].count().count()\n",
    "    # Number of time product has been bought by distinct user\n",
    "    df_freq=pd.DataFrame(prod_clust.groupby(['id'])['id'].count()/distinct_user)\n",
    "    df_freq.columns=['freq']\n",
    "    # Merge data\n",
    "    return pd.merge(bons_plans,df_freq,left_on='id',right_on=df_freq.index.values,how='left' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filterBonsPlans(bon_plans,user_id,ah,dict_aprio,matrix_user,dict_pond_mod):\n",
    "    matrix_product=createMatrixProduct(bon_plans)\n",
    "\n",
    "    # Selecting columns\n",
    "    matrix_user.index=matrix_user['user_id']\n",
    "    del matrix_user['user_id']\n",
    "    del matrix_user['age']\n",
    "    del matrix_user['sexe']\n",
    "    del matrix_user['situation']\n",
    "    del matrix_user['zone']\n",
    "    #print matrix_user.columns.values\n",
    "    matrix_product=matrix_product[matrix_user.columns.values]\n",
    "\n",
    "    # Distance calculation with ponderation\n",
    "    bon_plans['form_user_dist']=MinMaxScaler().fit_transform(pairwise_distances(matrix_product[matrix_user.columns], matrix_user, metric='cosine'))*float(dict_pond_mod['form_user'])\n",
    "\n",
    "    # Apriori Method with ponderation\n",
    "    bon_plans['apriori_rating']=MinMaxScaler().fit_transform(1-bon_plans.id.map(lambda x:dict_aprio[x] if x in dict_aprio else 0))*float(dict_pond_mod['hist_apriori'])\n",
    "\n",
    "    # History of the user with ponderation\n",
    "    if ah:\n",
    "        bon_plans['hist_user_dist']=MinMaxScaler().fit_transform(bon_plans['hist_user_dist'])*float(dict_pond_mod['hist_user'])\n",
    "    else:\n",
    "        bon_plans['hist_user_dist']=0\n",
    "    #print bon_plans['hist_user_dist']\n",
    "\n",
    "    # Formulaire cluster\n",
    "    if 'freq' in bon_plans:\n",
    "        bon_plans['freq']=bon_plans['freq'].fillna(0)\n",
    "        bon_plans['clust_rating']=MinMaxScaler().fit_transform(1-bon_plans['freq'])*float(dict_pond_mod['hist_clust'])\n",
    "    else :\n",
    "        bon_plans['clust_rating']=0\n",
    "        bon_plans['freq']=0\n",
    "\n",
    "    # Addition finale\n",
    "    bon_plans['dist_rating']=bon_plans['form_user_dist']+bon_plans['hist_user_dist']+bon_plans['apriori_rating']+bon_plans['clust_rating']\n",
    "\n",
    "    bon_plans['order']=bon_plans['analytics_userCategory']*(MinMaxScaler().fit_transform(-bon_plans['mntReduc']))\n",
    "    result=bon_plans.sort_values(by=['dist_rating','order','pctReduc','prix_old','prix_unitOff','travelTimeRaw'],ascending=[True, False,True,True,True,True])\n",
    "\n",
    "    # drop duplicate products\n",
    "    result['col_affich']=result['offreMarq']+' '+result['offreDesc']+' '+result['offreLib']+' '+result['libelle']\n",
    "    result['duplicated']=result.duplicated(subset=['col_affich'],keep='first')\n",
    "    result=result[result['duplicated']!=True]\n",
    "    del result['duplicated']\n",
    "    return result\n",
    "\n",
    "def mapToJson(bon_plans,features_df):\n",
    "    \"\"\"Conversion du dataframe de resultats en json pour etre envoye a apps\n",
    "\n",
    "    Args:\n",
    "        result: resultats avec le score des bons plans pour lutilisateur\n",
    "\n",
    "    Returns:\n",
    "        json de resultat\n",
    "    \"\"\"\n",
    "    res=[]\n",
    "    #print 'TAILLE :',features_df['nbBp'][0],bon_plans.shape\n",
    "    for nb_bp in range(min(int(features_df['features']['nbBp']),bon_plans.shape[0])):\n",
    "        res.append(bon_plans['id'].values[nb_bp])\n",
    "        #res.append(bon_plans['col_affich'].values[nb_bp])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveResult(features_df,bons_plans):\n",
    "    #output_ds = dataiku.Dataset('userBonsPlans',project_key='BONSPLANS')\n",
    "    df_to_save=pd.DataFrame(bons_plans)\n",
    "    df_to_save['userId']=features_df['features']['idUser']\n",
    "    df_to_save['date']=time()\n",
    "    df_to_save['clipped']=''\n",
    "    df_to_save['burned']=''\n",
    "    # Writing the data\n",
    "    del df_to_save['magCat']\n",
    "    del df_to_save['magHor']\n",
    "    del df_to_save['analytics_category_list']\n",
    "\n",
    "\n",
    "    #output_ds.write_from_dataframe(df_to_save)\n",
    "\n",
    "    time_msql=time()\n",
    "    # Connecting to DB\n",
    "    #time_connect_db=time()\n",
    "    conn= mysql.connector.connect(host='localhost',database='dataiku',user='dkuadmin',password='Dataiku!')\n",
    "    #print 'Time to connect to DB :', time()-time_connect_db\n",
    "\n",
    "    engine = create_engine('mysql+mysqlconnector://dkuadmin:Dataiku!@localhost:3306/dataiku', echo=False)\n",
    "    for i in range(df_to_save.shape[0]):\n",
    "        print pd.DataFrame(df_to_save.iloc[i,:]).transpose()\n",
    "        pd.DataFrame(df_to_save.iloc[i,:]).transpose().to_sql(name='userbonsplans', con=engine, if_exists='append', index=False, chunksize=200)\n",
    "\n",
    "    print 'Time mysql :',time()-time_msql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(features_df):\n",
    "\n",
    "    # Note: this sample uses the second form (decision_series, proba_df)\n",
    "\n",
    "    # Note: this sample \"cheats\" and always returns 5 predictions.\n",
    "    # You should actually return 1 prediction per row in the features_df\n",
    "\n",
    "    #print \"Features DataFrame %s\" % features_df\n",
    "\n",
    "    # Pondération Dictionary\n",
    "    dict_pond_mod=getDictPond()\n",
    "    print 'DICTIONARY :',dict_pond_mod\n",
    "\n",
    "    #  Get user info\n",
    "    time_gtuserinfo=time()\n",
    "    loc_user=locUser(features_df)\n",
    "    user_id=getUserId(features_df)\n",
    "    matrix_users=getUsersMatrix()\n",
    "    matrix_user=getUserMatrix(matrix_users,user_id)\n",
    "    mode_user=modeUser(matrix_user)\n",
    "    time_gtuserinf=time()-time_gtuserinfo\n",
    "    print 'Time get user infos : ', time_gtuserinf\n",
    "\n",
    "    # Get bons plans info\n",
    "    time_getbpinfos=time()\n",
    "    step_1=time()\n",
    "    step_1_i=mapFeatureDF(features_df)\n",
    "    print 'STEP 1 :', time()-step_1\n",
    "    step_2=time()\n",
    "    bons_plans=joinBPMag(step_1_i)\n",
    "    print 'STEP 2 :', time()-step_2\n",
    "    time_getbpinfos=time()-time_getbpinfos\n",
    "    print 'Time get bons plans infos : ', time_getbpinfos\n",
    "\n",
    "    if bons_plans.shape[0]>0:\n",
    "        # Enrich bons plans\n",
    "        time_enrichbonsplans=time()\n",
    "        bons_plans=enrichDataProduct(bons_plans, loc_user, mode_user, matrix_user)\n",
    "        time_enrich=time()-time_enrichbonsplans\n",
    "        print 'Time enrich bons plans infos : ', time_enrich\n",
    "\n",
    "        # Get historic buyings\n",
    "        time_simil=time()\n",
    "        # Get bought bons plans\n",
    "        old_plans_alluser=getHistoricBuy()\n",
    "        old_plans=getHistoryBuyFromUser(old_plans_alluser,features_df)\n",
    "        userAsHist=asHist(old_plans)\n",
    "        print 'USER AS HISTORY :', userAsHist\n",
    "        if userAsHist:\n",
    "            old_new_bp=mergeNewOldDB(bons_plans,features_df,old_plans)\n",
    "            res=similarityCalculation(old_new_bp)\n",
    "            res_scale=convertNeighborUnit(res)\n",
    "            result=getHistoricalWeight(old_new_bp, res_scale)\n",
    "            bons_plans=mergeScoringToBonsPlans(bons_plans,result)\n",
    "            del bons_plans['weight']\n",
    "        time_similarity=time()-time_simil\n",
    "        print 'Time calculate similarity : ', time_similarity\n",
    "\n",
    "        # Association Rule : APriori method\n",
    "        time_ar=time()\n",
    "        dict_res_aprio={}\n",
    "        if (dict_pond_mod['hist_apriori']>0) & (userAsHist):\n",
    "            data_hist=loadHistoricalData()\n",
    "            dataset, dataset_user=formatData(data_hist,user_id)\n",
    "            df_rule=aprioriMethod(dataset,0.1)\n",
    "            if df_rule.shape[0]>0:\n",
    "                df_select_aprio=aprioriSelection(df_rule,dataset_user)\n",
    "                dict_res_aprio=aprioriResults(df_select_aprio)\n",
    "        time_ar_fin=time()-time_ar\n",
    "        print 'Time Association Rule :',time_ar_fin\n",
    "\n",
    "        # Clustering on formulaire data\n",
    "        k_numb=3\n",
    "        if (matrix_users.shape[0]>=k_numb) & (dict_pond_mod['hist_clust']>0):\n",
    "            print 'GO INTO FORMULAIRE CLUSTER'\n",
    "            matrix_users=clusterKMeans(matrix_users,k_numb)\n",
    "            bons_plans=findBestProductByCluster(old_plans_alluser,user_id,matrix_users,bons_plans)\n",
    "\n",
    "        # Filtering of the bons plans\n",
    "        time_filtering=time()\n",
    "        bons_plans=filterBonsPlans(bons_plans,user_id,userAsHist,dict_res_aprio,matrix_user,dict_pond_mod)\n",
    "        #print bons_plans\n",
    "        time_filt=time()-time_filtering\n",
    "        print 'Time filtering :', time_filt\n",
    "\n",
    "        # Predictions, output\n",
    "        time_mappingres=time()\n",
    "        predictions = mapToJson(bons_plans,features_df)\n",
    "        time_map= time()-time_mappingres\n",
    "        print 'Time mapping result : ', time_map\n",
    "\n",
    "\n",
    "\n",
    "        # print results\n",
    "        time_saveRes=time()\n",
    "        # saveResult(features_df,bons_plans)\n",
    "        tim_sav=time()-time_saveRes\n",
    "        print 'Time saving result : ', tim_sav\n",
    "\n",
    "\n",
    "        # Synthesis time\n",
    "        print '*'*100\n",
    "        print 'Time get user infos : ', time_gtuserinf\n",
    "        print 'Time get bons plans infos : ', time_getbpinfos\n",
    "        print 'Time enrich bons plans infos : ', time_enrich\n",
    "        print 'Time calculate similarity : ', time_similarity\n",
    "        print 'Time Association Rule :',time_ar_fin\n",
    "        print 'Time filtering :', time_filt\n",
    "        print 'Time mapping result : ', time_map\n",
    "        print 'Time saving result : ', tim_sav\n",
    "        print '*'*50\n",
    "\n",
    "        print 'Time total procedure : ',time()-time_gtuserinfo\n",
    "        print '*'*100\n",
    "\n",
    "    # Case nothing into the bon plans\n",
    "    else:\n",
    "        predictions = pd.Series([json.dumps([''],indent=3)])\n",
    "\n",
    "    return (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DICTIONARY : {u'form_user': 0.1, u'hist_apriori': 0.0, u'hist_user': 0.5, u'hist_clust': 0.0, u'date': 20160922060136.0, u'svd_weight': 0.1}\n",
      "Time get user infos :  0.263776063919\n",
      "STEP 1 : 0.000153064727783\n",
      "complet :  319\n",
      "MAP BP : 0.0424978733063\n",
      "STEP 2 : 0.0588440895081\n",
      "Time get bons plans infos :  0.0592451095581\n",
      "Time Normalize : 0.109178066254\n",
      "TIME FROM DF TO INPUT : 0.115995883942\n",
      "PREDICT PICKLE 0.0247361660004\n",
      "TIME_FOR_CAT 0.149422168732\n",
      "Probleme with API Google : https://maps.googleapis.com/maps/api/distancematrix/json?origins=48.1125425,-1.680669&destinations=48.901790678501,2.304879873991|48.083015,-1.681273|48.123621,-1.659559|48.104252,-1.6793252|48.1059505,-1.6806113|48.1131261,-1.6804584|48.109855,-1.680353|48.10918,-1.67967|48.90166541851,2.3049399852753|48.112318664789,-1.6793632507324|48.109786,-1.689391|48.910604227267,2.2903699874878|48.88927,2.3048|48.910077721843,2.2957100868225|48.903691023588,2.3203870654106|48.1111683,-1.6829649|48.903323560953,2.3034569621086|48.8934157,2.2899788|48.113367408514,-1.6823941469193|48.1098529,-1.6667557|48.893217,2.287864|48.888088,2.311566|48.111116,-1.678255|48.8878528,2.3059544|48.88806,2.30649&mode=walking&language=fr&sensor=false&key=AIzaSyAw7VWZfZn0AUVeknT87QORTlZZxRIolZI\n",
      "TimeTempsTrajet : 0.120742082596\n",
      "Time enrich bons plans infos :  0.436452150345\n",
      "USER AS HISTORY : False\n",
      "Time calculate similarity :  0.239341974258\n",
      "Time Association Rule : 2.14576721191e-06\n",
      "TIME LOOP CREATE : 0.194725036621\n",
      "Time matrix_product : 0.203034162521\n",
      "Time filtering : 0.225351810455\n",
      "Time mapping result :  0.000513076782227\n",
      "Time saving result :  0.0\n",
      "****************************************************************************************************\n",
      "Time get user infos :  0.263776063919\n",
      "Time get bons plans infos :  0.0592451095581\n",
      "Time enrich bons plans infos :  0.436452150345\n",
      "Time calculate similarity :  0.239341974258\n",
      "Time Association Rule : 2.14576721191e-06\n",
      "Time filtering : 0.225351810455\n",
      "Time mapping result :  0.000513076782227\n",
      "Time saving result :  0.0\n",
      "**************************************************\n",
      "Time total procedure :  1.22766804695\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'aHR0cHM6Ly93d3cubW9ub3ByaXguZnIvcG9tbWUtYWJyaWNvdC1kZXMtNC02bW9pcy1zYW5zLXN1Y3Jlcy1ham91dGVzLWJsZWRpbmEtMTk3ODg4NS1wP2xhdC1sbmc9NDguMTA0MjUyLC0xLjY3OTMyNTI\\\\u003d',\n",
       " u'aHR0cHM6Ly93d3cubW9ub3ByaXguZnIvbGFpdC1kZW1pLWVjcmVtZS1zdGVyaWxpc2UtdWh0LWNlcnRpZmllLWFiLW1vbm9wcml4LWJpby05NjYxMzctcD9sYXQtbG5nPTQ4LjEwNDI1MiwtMS42NzkzMjUy',\n",
       " u'aHR0cHM6Ly93d3cubW9ub3ByaXguZnIvcGV0aXRzLXBvdHMtYS1sYS1wb21tZS1ldC1iYW5hbmUtZGVzLTQtbW9pcy1ibGVkaW5hLTE1MjY1OTItcD9sYXQtbG5nPTQ4LjEwNDI1MiwtMS42NzkzMjUy',\n",
       " u'aHR0cHM6Ly93d3cubW9ub3ByaXguZnIvcGV0aXRzLXBvdHMtY29rdGFpbC1kZS1mcnVpdHMtZGVzLTYtbW9pcy1ibGVkaW5hLTE1MjY2MDMtcD9sYXQtbG5nPTQ4LjEwNDI1MiwtMS42NzkzMjUy',\n",
       " u'aHR0cHM6Ly93d3cubW9ub3ByaXguZnIvbGFpdC1kZW1pLWVjcmVtZS1zdGVyaWxpc2UtdWh0LW1vbm9wcml4LTg3NTkwOS1wP2xhdC1sbmc9NDguMTA0MjUyLC0xLjY3OTMyNTI\\\\u003d',\n",
       " u'aHR0cHM6Ly93d3cubW9ub3ByaXguZnIvcG9tbWUtZnJhaXNlLWRlcy02bW9pcy1ibGVkaW5hLTIyNTg2MTgtcD9sYXQtbG5nPTQ4LjEwNDI1MiwtMS42NzkzMjUy',\n",
       " u'aHR0cDovL2NvdXJzZXMuY2FycmVmb3VyLmZyL2RyaXZlL3Byb21vdGlvbnMvdG91cy1sZXMtcmF5b25zL3lhb3VydHMtc3VyLWxpdC1kZS1wb2lyZXMtZ2VydmFpcy9QSUQxLzEyMzYxNi8xNTY1OTIyLzc~c2VydmljZURyaXZlPTMmc3RvcmVEcml2ZT0xNDIw',\n",
       " u'aHR0cDovL2NvdXJzZXMuY2FycmVmb3VyLmZyL2RyaXZlL3Byb21vdGlvbnMvdG91cy1sZXMtcmF5b25zL3lhb3VydHMtbGl0LWRlLW15cnRpbGxlcy1nZXJ2YWlzL1BJRDEvMTIzNjE2LzEyMTY3MTAvNz9zZXJ2aWNlRHJpdmU9MyZzdG9yZURyaXZlPTE0MjA\\\\u003d',\n",
       " u'aHR0cDovL2NvdXJzZXMuY2FycmVmb3VyLmZyL2RyaXZlL3Byb21vdGlvbnMvdG91cy1sZXMtcmF5b25zL3lhb3VydHMtbGl0LWQtYWdydW1lcy1nZXJ2YWlzL1BJRDEvMTIzNjE2LzEyMTY3MTEvNz9zZXJ2aWNlRHJpdmU9MyZzdG9yZURyaXZlPTE0MjA\\\\u003d',\n",
       " u'aHR0cDovL2NvdXJzZXMuY2FycmVmb3VyLmZyL2RyaXZlL3Byb21vdGlvbnMvdG91cy1sZXMtcmF5b25zL3BhaW5zLWF1LWNob2NvbGF0LWJyaW9jaGUtcGFzcXVpZXIvUElEMS8xMjM2MTYvMTM5MDA3MC83P3NlcnZpY2VEcml2ZT0zJnN0b3JlRHJpdmU9MTQyMA\\\\u003d\\\\u003d',\n",
       " u'aHR0cDovL2NvdXJzZXMuY2FycmVmb3VyLmZyL2RyaXZlL3Byb21vdGlvbnMvdG91cy1sZXMtcmF5b25zL2p1cy1kZS1wb21tZS1mcnVpdGUvUElEMS8xMjM2MTYvMTI2NzI2MS83P3NlcnZpY2VEcml2ZT0zJnN0b3JlRHJpdmU9MTQyMA\\\\u003d\\\\u003d',\n",
       " u'aHR0cDovL2NvdXJzZXMuY2FycmVmb3VyLmZyL2RyaXZlL3Byb21vdGlvbnMvdG91cy1sZXMtcmF5b25zL2Rlc3NlcnRzLXZhbmlsbGUtY2hvY29sYXQtZGFuZXR0ZS9QSUQxLzEyMzYxNi8xMzI0MzE3Lzc~c2VydmljZURyaXZlPTMmc3RvcmVEcml2ZT0xNDIw',\n",
       " u'aHR0cDovL2NvdXJzZXMuY2FycmVmb3VyLmZyL2RyaXZlL3Byb21vdGlvbnMvdG91cy1sZXMtcmF5b25zL2Nob3Jpem8tbm9lbC9QSUQxLzEyMzYxNi8xNjkxNTUxLzc~c2VydmljZURyaXZlPTMmc3RvcmVEcml2ZT0xNDIw',\n",
       " u'aHR0cDovL2NvdXJzZXMuY2FycmVmb3VyLmZyL2RyaXZlL3Byb21vdGlvbnMvdG91cy1sZXMtcmF5b25zL2Nob3Jpem8tZm9ydC1ib3JkZWF1LWNoZXNuZWwvUElEMS8xMjM2MTYvMTg4Mzg0OC83P3NlcnZpY2VEcml2ZT0zJnN0b3JlRHJpdmU9MTQyMA\\\\u003d\\\\u003d',\n",
       " u'aHR0cDovL2NvdXJzZXMuY2FycmVmb3VyLmZyL2RyaXZlL3Byb21vdGlvbnMvdG91cy1sZXMtcmF5b25zL2phbWJvbi1tYWRyYW5nZS9QSUQxLzEyMzYxNi8xMzk3NDE0Lzc~c2VydmljZURyaXZlPTMmc3RvcmVEcml2ZT0xNDIw',\n",
       " u'aHR0cDovL2NvdXJzZXMuY2FycmVmb3VyLmZyL2RyaXZlL3Byb21vdGlvbnMvdG91cy1sZXMtcmF5b25zL2NoaXBvbGF0YXMtZW5yb3VsZWUtY2hhcmN1dGVyaWVzLWR1LWRvbi1zdXBlcmlldXJlcy1iaWdhcmQvUElEMS8xMjM2MTYvOTQ1MzMyLzc~c2VydmljZURyaXZlPTMmc3RvcmVEcml2ZT0xNDIw',\n",
       " u'aHR0cDovL2NvdXJzZXMuY2FycmVmb3VyLmZyL2RyaXZlL3Byb21vdGlvbnMvdG91cy1sZXMtcmF5b25zL2phbWJvbi1zZWMtaXRhbGllbi1kZWxwZXlyYXQvUElEMS8xMjM2MTYvMTU2MzQyMy83P3NlcnZpY2VEcml2ZT0zJnN0b3JlRHJpdmU9MTQyMA\\\\u003d\\\\u003d',\n",
       " u'aHR0cDovL2NvdXJzZXMuY2FycmVmb3VyLmZyL2RyaXZlL3Byb21vdGlvbnMvdG91cy1sZXMtcmF5b25zL2JpZnRlY2tzLWNoYXJhbC9QSUQxLzEyMzYxNi83NTQyNjgvNz9zZXJ2aWNlRHJpdmU9MyZzdG9yZURyaXZlPTE0MjA\\\\u003d',\n",
       " u'aHR0cDovL2NvdXJzZXMuY2FycmVmb3VyLmZyL2RyaXZlL3Byb21vdGlvbnMvdG91cy1sZXMtcmF5b25zL3NhdW1vbi1mdW1lLWRlLW5vcnZlZ2UtbGFiZXlyaWUvUElEMS8xMjM2MTYvMTQyNTA4Ni83P3NlcnZpY2VEcml2ZT0zJnN0b3JlRHJpdmU9MTQyMA\\\\u003d\\\\u003d',\n",
       " u'aHR0cDovL2NvdXJzZXMuY2FycmVmb3VyLmZyL2RyaXZlL3Byb21vdGlvbnMvdG91cy1sZXMtcmF5b25zL3NhdW1vbi1mdW1lLWRlLW5vcnZlZ2UtYXJvbWF0ZXMtbGFiZXlyaWUvUElEMS8xMjM2MTYvMTExNDUzMy83P3NlcnZpY2VEcml2ZT0zJnN0b3JlRHJpdmU9MTQyMA\\\\u003d\\\\u003d',\n",
       " u'aHR0cHM6Ly93d3cubW9ub3ByaXguZnIvZWF1LW1pbmVyYWxlLW5hdHVyZWxsZS12b2x2aWMtMjQ5Njc0Ny1wP2xhdC1sbmc9NDguMTA0MjUyLC0xLjY3OTMyNTI\\\\u003d',\n",
       " u'aHR0cHM6Ly93d3cubW9ub3ByaXguZnIvZWF1LW1pbmVyYWxlLW5hdHVyZWxsZS1nYXpldXNlLXNhbi1wZWxsZWdyaW5vLTk1MjY3Ni1wP2xhdC1sbmc9NDguMTA0MjUyLC0xLjY3OTMyNTI\\\\u003d',\n",
       " u'aHR0cDovL2NvdXJzZXMuY2FycmVmb3VyLmZyL2RyaXZlL3Byb21vdGlvbnMvdG91cy1sZXMtcmF5b25zL2Jpc2N1aXRzLWJydXQtNS1jZXJlYWxlcy1jb21wbGV0ZXMtYmVsdml0YS9QSUQxLzEyMzYxNi8xNDc5MzIxLzc~c2VydmljZURyaXZlPTMmc3RvcmVEcml2ZT0xNDIw',\n",
       " u'aHR0cDovL2NvdXJzZXMuY2FycmVmb3VyLmZyL2RyaXZlL3Byb21vdGlvbnMvdG91cy1sZXMtcmF5b25zL2Jpc2N1aXRzLWFwZXJpdGlmLW1vbmFjby1taW5penphLXRvbWF0ZS1iZWxpbi9QSUQxLzEyMzYxNi8xNzczOTgyLzc~c2VydmljZURyaXZlPTMmc3RvcmVEcml2ZT0xNDIw',\n",
       " u'aHR0cDovL2NvdXJzZXMuY2FycmVmb3VyLmZyL2RyaXZlL3Byb21vdGlvbnMvdG91cy1sZXMtcmF5b25zL2Jpc2N1aXRzLWFwZXJpdGlmLW1vbmFjby1mZXVpbGxldGUtZW1tZW50YWwtYmVsaW4vUElEMS8xMjM2MTYvMTc3Mzk4My83P3NlcnZpY2VEcml2ZT0zJnN0b3JlRHJpdmU9MTQyMA\\\\u003d\\\\u003d',\n",
       " u'aHR0cDovL2NvdXJzZXMuY2FycmVmb3VyLmZyL2RyaXZlL3Byb21vdGlvbnMvdG91cy1sZXMtcmF5b25zL2ZhcmluZS1kZS1ibGUtcGF0aXNzZXJpZS1jdWlzaW5lLWVibHkvUElEMS8xMjM2MTYvODcwNDYzLzc~c2VydmljZURyaXZlPTMmc3RvcmVEcml2ZT0xNDIw',\n",
       " u'aHR0cDovL2NvdXJzZXMuY2FycmVmb3VyLmZyL2RyaXZlL3Byb21vdGlvbnMvdG91cy1sZXMtcmF5b25zL3NhbGFkZS1iYXRhdmlhLWxlcy1jcnVkZXR0ZXMvUElEMS8xMjM2MTYvNTM5ODc5Lzc~c2VydmljZURyaXZlPTMmc3RvcmVEcml2ZT0xNDIw',\n",
       " u'aHR0cDovL2NvdXJzZXMuY2FycmVmb3VyLmZyL2RyaXZlL3Byb21vdGlvbnMvdG91cy1sZXMtcmF5b25zL2phbWJvbi1sZS1ib24tcGFyaXMtaGVydGEvUElEMS8xMjM2MTYvMTg1MzUxNS83P3NlcnZpY2VEcml2ZT0zJnN0b3JlRHJpdmU9MTQyMA\\\\u003d\\\\u003d',\n",
       " u'aHR0cDovL2NvdXJzZXMuY2FycmVmb3VyLmZyL2RyaXZlL3Byb21vdGlvbnMvdG91cy1sZXMtcmF5b25zL3Zpbi1yb3NlLW1lcmxvdC1wYXlzLWQtb2MtMjAxNS1sZXMtb3JtZXMtZGUtY2FtYnJhcy9QSUQxLzEyMzYxNi8xODgxNDY4Lzc~c2VydmljZURyaXZlPTMmc3RvcmVEcml2ZT0xNDIw',\n",
       " u'aHR0cDovL2NvdXJzZXMuY2FycmVmb3VyLmZyL2RyaXZlL3Byb21vdGlvbnMvdG91cy1sZXMtcmF5b25zL3Zpbi1yb3VnZS1tZXJsb3QtcGF5cy1kLW9jLTIwMTUtbGVzLW9ybWVzLWRlLWNhbWJyYXMvUElEMS8xMjM2MTYvMzM4OTcyLzc~c2VydmljZURyaXZlPTMmc3RvcmVEcml2ZT0xNDIw']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df = json.loads(open('../inputs_bouchons/bons_plans.json').read())\n",
    "predict(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
