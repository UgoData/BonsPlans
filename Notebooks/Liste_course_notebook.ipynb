{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "import json, requests\n",
    "import re,sys\n",
    "#from dataiku import Folder\n",
    "import pickle\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.spatial.distance import cosine\n",
    "from classCustom import ItemSelector, Scalers\n",
    "from time import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "import unicodedata\n",
    "from Normalizer import *\n",
    "from EnrichProduct import *\n",
    "import mysql.connector\n",
    "import MySQLdb\n",
    "from sqlalchemy import create_engine\n",
    "from pandas.io.json import json_normalize\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models_path='Models/multiNB.p'\n",
    "model = pickle.load(open(models_path, \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "################################## Environnement variables ###################################\n",
    "##############################################################################################\n",
    "\n",
    "list_stop_word_french=['alors','au','aucuns','aussi','autre','avant','avec','avoir','bon','car','ce','cela','ces','ceux','chaque',\n",
    "                       'ci','comme','comment','dans','des','du','dedans','dehors','depuis','devrait','doit','donc','dos','début',\n",
    "                       'elle','elles','en','encore','essai','est','et','eu','fait','faites','fois','font','hors','ici','il','ils',\n",
    "                       'je','juste','la','le','les','leur','là','ma','maintenant','mais','mes','mine','moins','mon','mot','même',\n",
    "                       'ni','nommés','notre','nous','ou','où','par','parce','pas','peut','peu','plupart','pour','pourquoi','quand',\n",
    "                       'que','quel','quelle','quelles','quels','qui','sa','sans','ses','seulement','si','sien','son','sont','sous',\n",
    "                       'soyez','sujet','sur','ta','tandis','tellement','tels','tes','ton','tous','tout','trop','très','tu','voient',\n",
    "                       'vont','votre','vous','vu','ça','étaient','état','étions','été','être']\n",
    "# Suppress number\n",
    "reg_numb = re.compile('[^\\D]')\n",
    "# Suppress punctuation\n",
    "reg_ponct = re.compile('[^a-z 0-9ÀÁÂÃÄÅàáâãäåÒÓÔÕÖØòóôõöøÈÉÊËèéêëÇçÌÍÎÏìíîïÙÚÛÜùúûüÿÑñ²°Ø×ßŠ”�œ…]')\n",
    "# Suppress stop words\n",
    "french_stopwords_ini=stopwords.words('french')\n",
    "french_stopwords_ini.extend(list_stop_word_french)\n",
    "french_stopwords = set(french_stopwords_ini)\n",
    "# Stemming of words\n",
    "stemmer = FrenchStemmer()\n",
    "\n",
    "list_cat = ['1001','1002','1003','1004','1005','1006','1007','1008','1010','1011',\n",
    "            '2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get ponderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "######################## Recuperation des pondérations des methodes ##########################\n",
    "##############################################################################################\n",
    "\n",
    "### Dictionnaire de ponderation des modeles\n",
    "def getDictPond():\n",
    "    conn= mysql.connector.connect(host='localhost',database='dataiku',user='dkuadmin',password='Dataiku!')\n",
    "    req = \"SELECT * from pondlistecourse order by date desc limit 1\"\n",
    "    pond_df = pd.read_sql(req, conn)\n",
    "    conn.close()\n",
    "    pond_df_temp=pond_df.to_dict(orient='list')\n",
    "    return {k:v[0] for (k,v) in pond_df_temp.iteritems()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get product infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "########################### Recuperation des donnees des produits ############################\n",
    "##############################################################################################\n",
    "\n",
    "def mapFeatureDF(features_df):\n",
    "    \"\"\" map features to user_info \"\"\"\n",
    "    return features_df['features']['listeCourse'],features_df['features']['listeMagasins'],features_df['features']['listeMarchands']\n",
    "\n",
    "def getCouponsData(rawJsonLC):\n",
    "    \"\"\" Create a dataframe with all the produts informations\"\"\"\n",
    "\n",
    "    # Set empty dataframe with all possible columns\n",
    "    df_empty_allcol=pd.DataFrame(columns=['bons-plans','description','distance','id','image','lat-lng','libelle','magasin','marque','metadata.prix unitaire','metadata.source','metadata.unité','position','prix','requete'])\n",
    "\n",
    "    for i in rawJsonLC[0].keys():\n",
    "        if len(rawJsonLC[0][i])>0:\n",
    "            df=json_normalize(rawJsonLC[0][i])\n",
    "            df['requete']=i\n",
    "            try:\n",
    "                df['reduc']=df['bons-plans'].map(lambda x:x[0]['libelle'] if len(x)>0 else \"\")\n",
    "            except:\n",
    "                df['reduc']=''\n",
    "            # Merge to be sure having all the columns\n",
    "            df_empty_allcol=df_empty_allcol.append(df, ignore_index=True)\n",
    "\n",
    "    print (df_empty_allcol.index)\n",
    "    if len(df_empty_allcol.index) >0:\n",
    "        # Keep only interesting columns\n",
    "        result=df_empty_allcol[['requete','libelle','description','prix','id','reduc','magasin','image','distance','position','metadata.prix unitaire','metadata.unité','metadata.source']]\n",
    "        product_info=pd.DataFrame(result.values,columns=['requete','nomProduit','descriptionProduit','prix','id',\n",
    "                           'reduc',\n",
    "                           'magId',\n",
    "                           'urlImage',\n",
    "                           'distance',\n",
    "                           'position','prixUnit','unit','source'])\n",
    "        product_info['prixMin']=0\n",
    "        product_info['prixMax']=1000\n",
    "        # Fill na\n",
    "        product_info.fillna('',inplace=True)\n",
    "        # Encoding\n",
    "        product_info.nomProduit=product_info.nomProduit.str.encode('utf-8')\n",
    "        product_info.descriptionProduit=product_info.descriptionProduit.str.encode('utf-8')\n",
    "        product_info.requete=product_info.requete.str.encode('utf-8')\n",
    "        product_info.reduc=product_info.reduc.str.encode('utf-8')\n",
    "        product_info.unit=product_info.unit.str.encode('utf-8')\n",
    "        product_info.source=product_info.source.str.encode('utf-8')\n",
    "    else :\n",
    "        product_info=pd.DataFrame(columns=['requete','nomProduit','descriptionProduit','prix','id',\n",
    "                           'reduc',\n",
    "                           'magId',\n",
    "                           'urlImage',\n",
    "                           'distance',\n",
    "                           'position','prixUnit','unit','source'])\n",
    "\n",
    "    return product_info\n",
    "\n",
    "def mapMagasins(rawJsonLC):\n",
    "    \"\"\" Mapping des donnees sur les magasins \"\"\"\n",
    "    magasins=pd.DataFrame(columns=['magId','magZipcode','magDepcom',\n",
    "                                    'magLatLong','magRue',\n",
    "                                    'magCat','magHor',\n",
    "                                    'magMarch','magNom','magRmw'])\n",
    "    count=0\n",
    "    for i in range(len(rawJsonLC[1])):\n",
    "        sub_bp=rawJsonLC[1][i]\n",
    "        codP=''\n",
    "        comM=''\n",
    "        latLo=''\n",
    "        ruE=''\n",
    "        catE=''\n",
    "        horA=''\n",
    "        marC=''\n",
    "        noM=''\n",
    "        rmW=''\n",
    "        try:\n",
    "            codP=sub_bp['adresse']['code-postal']\n",
    "        except:\n",
    "            codP=''\n",
    "        try:\n",
    "            comM=sub_bp['adresse']['commune']\n",
    "        except:\n",
    "            comM=''\n",
    "        try:\n",
    "            latLo=sub_bp['adresse']['lat-lng']\n",
    "        except:\n",
    "            latLo=''\n",
    "        try:\n",
    "            ruE=sub_bp['adresse']['rue']\n",
    "        except:\n",
    "            ruE=''\n",
    "        try:\n",
    "            catE=sub_bp['categories']\n",
    "        except:\n",
    "            catE=''\n",
    "        try:\n",
    "            horA=sub_bp['horaires']\n",
    "        except:\n",
    "            horA=''\n",
    "        try:\n",
    "            marC=sub_bp['marchand']\n",
    "        except:\n",
    "            marC=''\n",
    "        try:\n",
    "            noM=sub_bp['nom'].encode('utf-8')\n",
    "        except:\n",
    "            noM=''\n",
    "        try:\n",
    "            rmW=sub_bp['rmw']\n",
    "        except:\n",
    "            rmW=''\n",
    "\n",
    "        magasins.loc[i]=[sub_bp['id'],codP,comM,\n",
    "                          latLo,ruE,\n",
    "                          catE,horA,\n",
    "                          marC,noM,rmW]\n",
    "\n",
    "        count+=1\n",
    "    return magasins\n",
    "\n",
    "def mapMarchands(rawJsonLC):\n",
    "    \"\"\" Mapping des donnees sur les marchands \"\"\"\n",
    "    marchands=pd.DataFrame(columns=['marchId','magMarch'])\n",
    "    count=0\n",
    "    for i in range(len(rawJsonLC)):\n",
    "        sub_bp=rawJsonLC[1][i]\n",
    "        marchands.loc[i]=[sub_bp['id'],sub_bp['nom'].encode('utf-8')]\n",
    "\n",
    "        count+=1\n",
    "    return marchands\n",
    "\n",
    "def joinBPMag(rawJsonLC, product_info_raw):\n",
    "    return pd.merge(product_info_raw, mapMagasins(rawJsonLC), on='magId',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empty product matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "###################### Creation de la matrice qualification des produits #####################\n",
    "############################################################################################## \n",
    "\n",
    "def matrixProductColumnsAndMajProducts(product_info,list_user,list_cat):\n",
    "    list_index=[]\n",
    "    for i in list_user:\n",
    "        for ii in list_cat.keys():\n",
    "            if i=='prixSens':\n",
    "                list_index.append(str(i)+'_'+str(ii)+'_highPrice')\n",
    "                list_index.append(str(i)+'_'+str(ii)+'_lowPrice')\n",
    "                product_info[str(i)+'_'+str(ii)+'_highPrice']=0\n",
    "                product_info[str(i)+'_'+str(ii)+'_lowPrice']=0\n",
    "            elif i=='reductionSens':\n",
    "                list_index.append(str(i)+'_'+str(ii)+'_noreduc')\n",
    "                list_index.append(str(i)+'_'+str(ii)+'_freeImm')\n",
    "                list_index.append(str(i)+'_'+str(ii)+'_2iemGrat')\n",
    "                list_index.append(str(i)+'_'+str(ii)+'_3iemGrat')\n",
    "                list_index.append(str(i)+'_'+str(ii)+'_carte')\n",
    "                list_index.append(str(i)+'_'+str(ii)+'_autres')\n",
    "                product_info[str(i)+'_'+str(ii)+'_noreduc']=0\n",
    "                product_info[str(i)+'_'+str(ii)+'_freeImm']=0\n",
    "                product_info[str(i)+'_'+str(ii)+'_2iemGrat']=0\n",
    "                product_info[str(i)+'_'+str(ii)+'_3iemGrat']=0\n",
    "                product_info[str(i)+'_'+str(ii)+'_carte']=0\n",
    "                product_info[str(i)+'_'+str(ii)+'_autres']=0\n",
    "            else:\n",
    "                list_index.append(str(i)+'_'+str(ii))\n",
    "                product_info[str(i)+'_'+str(ii)]=0\n",
    "    list_index.append('quantity_unit')\n",
    "    list_index.append('quantity_family')\n",
    "    list_index.append('travelTime')\n",
    "    list_index.append('mag_Fnac')\n",
    "    list_index.append('mag_Carrefour')\n",
    "    list_index.append('mag_Monoprix')\n",
    "    list_index.append('mag_Autres')\n",
    "    product_info['quantity_unit']=0\n",
    "    product_info['quantity_family']=0\n",
    "    product_info['travelTime']=0\n",
    "    product_info['mag_Fnac']=0\n",
    "    product_info['mag_Carrefour']=0\n",
    "    product_info['mag_Monoprix']=0\n",
    "    product_info['mag_Autres']=0\n",
    "\n",
    "    return list_index\n",
    "\n",
    "def matrixProductInit(list_index):\n",
    "    return pd.DataFrame(index=list_index)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "########################## Recuperation des donnees sur lutilisateur #########################\n",
    "############################################################################################## \n",
    "    \n",
    "def locUser(features_df):\n",
    "    return features_df['features']['localisationUser']\n",
    "\n",
    "def getUserId(features_df):\n",
    "    return features_df['features']['idUser']\n",
    "\n",
    "def modeUser(user_info):\n",
    "    return ['walking'] #'bicycling','walking','transit','driving'\n",
    "\n",
    "# Tester features mapping ou connecteur mysql direct\n",
    "def getUsersMatrix(user_id):\n",
    "    time_mysql=time()\n",
    "    conn= mysql.connector.connect(host='localhost',database='dataiku',user='dkuadmin',password='Dataiku!')\n",
    "    req = \"SELECT * from userinformations where user_id=\\'\"+user_id+\"\\'\"\n",
    "    couponscores = pd.read_sql(req, conn)\n",
    "    conn.close()\n",
    "    print 'Time mysql :',time()-time_mysql\n",
    "    return couponscores\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get product categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "################################# Get categories des produits ################################\n",
    "##############################################################################################\n",
    "\n",
    "    \n",
    "#### Get pickle and fit new data ####\n",
    "    \n",
    "def getCategory(df_topredict):\n",
    "    \"\"\"Get Categories of the product resulting from classification multi-label\n",
    "    Return : prediction in columns category order\"\"\"\n",
    "\n",
    "    # Prepare dataset to predict\n",
    "    df_topredict=fromDFToInput(df_topredict)\n",
    "\n",
    "    # Predict result\n",
    "    predict_time=time()\n",
    "    result_raw=model.predict(df_topredict)\n",
    "    print 'PREDICT PICKLE', time()-predict_time\n",
    "    result = pipeline_output_formatting(result_raw, list_cat)\n",
    "\n",
    "    # Cleaning df to predict\n",
    "    del df_topredict['NomDescr']\n",
    "    df_topredict['analytics_category']=result\n",
    "    #print 'Categories differentes :', df_topredict['analytics_category'].unique()\n",
    "    return df_topredict\n",
    "\n",
    "def fromDFToInput(df):\n",
    "    df.fillna('', inplace=True)\n",
    "\n",
    "    time_norm=time()            \n",
    "\n",
    "    df['nomProduit']=df['nomProduit'].apply(lambda x:normaliz(x,french_stopwords,reg_numb,reg_ponct,stemmer))\n",
    "    df['descriptionProduit']=df['descriptionProduit'].apply(lambda x:normaliz(x,french_stopwords,reg_numb,reg_ponct,stemmer))\n",
    "\n",
    "    df['NomDescr']=df['nomProduit']+' '+df['descriptionProduit']\n",
    "\n",
    "    return df\n",
    "\n",
    "### Formatting the output of the pipeline\n",
    "def pipeline_output_formatting(result_raw, list_cat):\n",
    "    result = []\n",
    "    for x in result_raw:\n",
    "        temp = []\n",
    "        for idy, y in enumerate(x):\n",
    "            if y>0:\n",
    "                temp.append(list_cat[idy])\n",
    "        result.append(temp)\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrich product information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "######################### Enrichissement des donnees sur les produits ########################\n",
    "##############################################################################################\n",
    "\n",
    "\n",
    "### Add columns to result for transposition into matrix_product ###\n",
    "def enrichDataProduct(product_info, locUser, modeUser):\n",
    "\n",
    "    # Add travel time\n",
    "    product_info=getTpsTrajet(product_info,locUser,modeUser)\n",
    "    #print 'Temps trajet :', product_info['walking'].unique()\n",
    "    # Add bio and surgele\n",
    "    product_info['bio']=(product_info['nomProduit'].str.contains(\"bio\")|product_info['nomProduit'].str.contains(\"certifie AB\"))\n",
    "    #print 'Bio :', product_info['bio'].unique()\n",
    "    product_info['surgele']=product_info['nomProduit'].str.contains(\"surgele\")\n",
    "    # Caculate new columns of quantity and unity\n",
    "    product_info['unite']=product_info.descriptionProduit.map(lambda x:getUnit(x).replace(' ',''))\n",
    "    product_info['quantite']=product_info.descriptionProduit.map(lambda x:getQuantity(x))\n",
    "    product_info['unite_val']=product_info.unite.map(lambda x:dictUnit(x))\n",
    "    product_info['quantite_unite']=product_info['unite_val']*product_info['quantite']\n",
    "    product_info['price_unit']=product_info['prix']/product_info['quantite_unite']\n",
    "    product_info['price_unit']=product_info.apply(bestPriceUnit,axis=1)\n",
    "    #print 'Price unit :', product_info['price_unit'].unique()\n",
    "    del product_info['unite_val']\n",
    "\n",
    "    # Add column analytics category\n",
    "    product_info=getCategory(product_info)   \n",
    "\n",
    "    # Quantile\n",
    "    #product_info['quantile_prixraw']=product_info.groupby(['requete'])['prix'].apply(lambda x: qcut_custom(x,4))\n",
    "    product_info['quantile_prixraw']=0\n",
    "    product_info['quantile_prixquantite']=product_info.groupby(['requete'])['price_unit'].apply(lambda x: pct_rank_qcut(x,6))\n",
    "    product_info['quantile_prixquantite']=product_info['quantile_prixquantite'].to_dict().values()\n",
    "    # Reduction type\n",
    "    product_info['reduc_type']=product_info['reduc'].map(lambda x:getReducType(x))\n",
    "\n",
    "    # Univers de consommation\n",
    "    regex = re.compile('[\\[\\] ]')\n",
    "    product_info['analytics_category_list']=product_info['analytics_category']\n",
    "    product_info['analytics_category_count']=product_info['analytics_category_list'].map(lambda x:len(x))\n",
    "    #print 'LISTANALYTICS', type(product_info['analytics_category_list'][0])\n",
    "\n",
    "    # Enrichissement des donnees des produits pour quils correspondent avec la matrice utilisateur\n",
    "    listCatProd=['1000','1001','1002','1003','1004','1005','1006','1007','1008','1009','1010','1011',\n",
    "                 '2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012']\n",
    "\n",
    "    for i in listCatProd:\n",
    "        # Univers Conso\n",
    "        product_info['universConso_'+str(i)]=product_info['analytics_category_list'].map(lambda x:1 if i in x else 0)/product_info['analytics_category_count']\n",
    "        # Price sensitivity\n",
    "        product_info['prixSens_'+str(i)+'_lowPrice']=product_info['quantile_prixquantite'].map(lambda x:float(abs(x-3))/2 if x<4 else 0)*product_info['universConso_'+str(i)]\n",
    "        product_info['prixSens_'+str(i)+'_highPrice']=product_info['quantile_prixquantite'].map(lambda x:float((x%4))/2 if x>3 else 0)*product_info['universConso_'+str(i)]\n",
    "        # Reduction Sensitivity\n",
    "        list_red=['carte','noreduc','2iemGrat','3iemGrat','freeImm','autres']\n",
    "        for y in list_red:\n",
    "            product_info['reductionSens_'+str(i)+'_'+y]=product_info['reduc_type'].map(lambda x:1 if x==y else 0)*product_info['universConso_'+str(i)]\n",
    "\n",
    "        # Bio Sensitiv\n",
    "        product_info['bioSens_'+str(i)]=product_info['bio']*product_info['universConso_'+str(i)]\n",
    "    del product_info['analytics_category_count']\n",
    "    # Magasin\n",
    "    product_info['mag_Fnac']=product_info['magNom'].map(lambda x:1 if 'fnac' in x.lower() else 0)\n",
    "    product_info['mag_Carrefour']=product_info['magNom'].map(lambda x:1 if 'carrefour' in x.lower() else 0)\n",
    "    product_info['mag_Monoprix']=product_info['magNom'].map(lambda x:1 if 'monop' in x.lower() else 0)\n",
    "    product_info['mag_Autres']=product_info['magNom'].map(lambda x:1 if 'other' in x.lower() else 0)    \n",
    "\n",
    "\n",
    "    return product_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating similarities with past buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "############################ Calculating similarities with past buy ##########################\n",
    "##############################################################################################\n",
    "\n",
    "def getHistoricBuy(features_df):\n",
    "    \"\"\" Get all the past buyings of the user. Put them into a database\"\"\"\n",
    "    time_history=time()\n",
    "    # Connecting to DB\n",
    "    conn= mysql.connector.connect(host='localhost',database='dataiku',user='dkuadmin',password='Dataiku!')\n",
    "\n",
    "    req='select * from dataiku.userlistecourse where (clipped=True or burned=True) and userId=\\''+features_df['features']['idUser']+'\\''\n",
    "    df_buy = pd.read_sql(req, conn)\n",
    "    conn.close()\n",
    "    #print 'Time get History :',time()-time_history\n",
    "    return df_buy\n",
    "\n",
    "def asHist(old_df):\n",
    "    \"\"\" Return True if the user has history\"\"\"\n",
    "    result=False\n",
    "    if old_df.shape[0]>0:\n",
    "        result=True\n",
    "    return result\n",
    "\n",
    "def mergeNewOldDB(bon_plans,features_df,old_plans):\n",
    "    \"\"\" Merge the new bons plans and the ones bought in the pass by the user\"\"\"\n",
    "    # Get weight\n",
    "    old_plans['weight']=old_plans.apply(getWeights,axis=1)\n",
    "    #print old_plans['weight']\n",
    "\n",
    "    op=old_plans[['idProd','nomProduit','descriptionProduit','prix','weight']]\n",
    "    op['from']='old'\n",
    "    # Get new bons plans\n",
    "    bon_plans['weight']=0\n",
    "    np=bon_plans[['id','nomProduit','descriptionProduit','prix','weight']]\n",
    "    np['from']='new'\n",
    "    # Merge data\n",
    "    return pd.concat([op,np])\n",
    "\n",
    "def similarityCalculation(df):\n",
    "    \"\"\"Calculate the similarity between products\"\"\"\n",
    "\n",
    "    # Creating common index\n",
    "    df['id2']=range(df.shape[0])\n",
    "    df.set_index('id2',inplace=True)\n",
    "\n",
    "    # Create a full description columns\n",
    "    df.loc[:,'fulldescriptionProduit']=pd.Series(df.loc[:,'nomProduit'].str.cat(df.loc[:,'descriptionProduit'].values, sep=' '),index=df.index)\n",
    "\n",
    "\n",
    "    # Scale the price with MinMaxScaler : price is in [0,1] range (for the distance)\n",
    "    # Price is very low for the majority of data => log to recenter the distribution\n",
    "    scaler = MinMaxScaler()\n",
    "    df['prix']=df['prix'].map(lambda x:x if x>0 else 0.00001)\n",
    "    df.loc[:,'scaledlogprix']=scaler.fit_transform(np.log(df.loc[:,'prix'].values).reshape(-1, 1))\n",
    "    #print df\n",
    "\n",
    "    # TFIDF\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=0.001,max_df=0.8,ngram_range=(1,2))\n",
    "    tfidf_productDescr=tfidf_vectorizer.fit_transform(df['fulldescriptionProduit'])\n",
    "\n",
    "    ## Concatenate index / TFIDF product name / Product price - Dense matrix\n",
    "    dist_data_dense=np.hstack((df.index.values.reshape(-1,1),tfidf_productDescr.toarray(),df.loc[:,'scaledlogprix'].values.reshape(-1,1)))\n",
    "\n",
    "    def custom_distance(x, y):\n",
    "        \"\"\" Creating the custom distance function between records\"\"\"\n",
    "        i, j = int(x[0]), int(y[0])  # extract index which is id from pandas df\n",
    "        # Computes cosine similarity on tf idf features (all features execpt id (0) and price (-1))\n",
    "        tfidf_dist=cosine(dist_data_dense[i,1:-1],dist_data_dense[j,1:-1])\n",
    "        # Price distance - absolute values\n",
    "        price_distance=np.abs(dist_data_dense[i,-1]-dist_data_dense[j,-1])\n",
    "        return tfidf_dist+price_distance\n",
    "\n",
    "    ## Distinction between old and new products\n",
    "    nb_old=df[df['from']=='old'].shape[0]\n",
    "    nb_new=df.shape[0]-nb_old\n",
    "    print 'NB_OLD and NB_NEW:',nb_old, nb_new\n",
    "\n",
    "\n",
    "    ## Distance pairwise\n",
    "    dist_df=pd.DataFrame(dist_data_dense)\n",
    "    return pd.DataFrame(pairwise_distances(dist_df.iloc[0:nb_old],dist_df.iloc[nb_old:],metric=custom_distance).transpose(),columns=['Row'+str(i) for i in range(1, nb_old+1)],index=df.id[nb_old:])\n",
    "\n",
    "def convertNeighborUnit(df):\n",
    "    \"\"\" Convert the 0: close to 2:far unit into a 0:far to 1:close unit\"\"\"\n",
    "    for i in df.columns:\n",
    "        #for y in range(len(df[i])):\n",
    "            #df.ix[y,i]=fabs(df.ix[y,i]-2)/2\n",
    "        df[i]=df[i].apply(lambda x:float(x)/2)\n",
    "    return df\n",
    "\n",
    "def getWeights(df):\n",
    "    result=0\n",
    "    if df['burned']:\n",
    "        result=1\n",
    "    elif df['clipped']:\n",
    "        result=.5\n",
    "    return result\n",
    "\n",
    "def getHistoricalWeight(df_with_weight, df_with_neight):\n",
    "    # Get weight clipped or burned\n",
    "    df_weight=df_with_weight.ix[df_with_weight['weight']>0,'weight']\n",
    "    # multiplication weight with neightbor\n",
    "    df_res=pd.DataFrame(df_with_neight.values*df_weight.transpose().values, columns=df_with_neight.columns, index=df_with_neight.index)\n",
    "    df_res['hist_user_dist']=df_res.sum(axis=1)/df_res.shape[1]\n",
    "\n",
    "    return df_res\n",
    "\n",
    "def mergeScoringToListeCourse(product_info,df_score):\n",
    "    \"\"\" Merge the scoring values to the bons plans \"\"\"\n",
    "    #print product_info.columns\n",
    "    #print df_score.columns\n",
    "    df_score2=df_score[['hist_user_dist']]\n",
    "    return pd.merge(product_info, df_score2, left_on='id',right_on=df_score2.index.values,how='left')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity with request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "################################### Similarity with request ##################################\n",
    "##############################################################################################\n",
    "    \n",
    "def similarityRequest(df):\n",
    "    \"\"\"Calculate the similarity between the request and the responses\"\"\"\n",
    "    result=pd.DataFrame(columns=['requete','nomProduit','id','dist_req'])\n",
    "    #print df.columns\n",
    "    for i in df['requete'].unique():\n",
    "        # df_temp\n",
    "        df_temp=df[['requete','nomProduit','id']]\n",
    "        df_temp=df_temp[df_temp['requete']==i]\n",
    "        df_temp=pd.DataFrame(np.array([[i, normaliz(i,french_stopwords,reg_numb,reg_ponct,stemmer), 'requete']])\n",
    "                             , columns=['requete','nomProduit','id']).append(df_temp, ignore_index=True)\n",
    "        # First words of the result\n",
    "        len_req=len(i.split(' '))\n",
    "        df_temp['shortNom']=df_temp['nomProduit'].map(lambda x:' '.join(x.split(' ')[0:len_req]))\n",
    "        # TFIDF\n",
    "        tfidf_vectorizer = TfidfVectorizer(min_df=0.001,max_df=0.8,ngram_range=(1,1))\n",
    "        try:\n",
    "            #print 'GO INTO REQUEST SIMILARITY'\n",
    "            tfidf_productDescr=tfidf_vectorizer.fit_transform(df_temp['shortNom'])\n",
    "            ## Distance cosinus\n",
    "            dist_df=pd.DataFrame(tfidf_productDescr.toarray())\n",
    "            res_temp=pairwise_distances(dist_df.iloc[0:1],dist_df.iloc[1:],metric='euclidean')\n",
    "            df_temp=df_temp[df_temp['id']!='requete']\n",
    "            df_temp.loc[:,'dist_req']=res_temp.transpose()\n",
    "            df_temp.loc[:,'dist_req']=1-MinMaxScaler().fit_transform(df_temp.loc[:,'dist_req'])\n",
    "            #print 'RES TEMP :', res_temp\n",
    "        except:\n",
    "            df_temp.loc[:,'dist_req']=0\n",
    "        result=result.append(df_temp,ignore_index=True)\n",
    "    return result\n",
    "\n",
    "def addReqDistCol(df):\n",
    "    result_dist=similarityRequest(df)\n",
    "    result_dist.set_index('id',inplace=True)\n",
    "    del result_dist['nomProduit']\n",
    "    del result_dist['shortNom']\n",
    "    del result_dist['requete']\n",
    "    return pd.merge(df,result_dist,left_on='id',right_on=result_dist.index.values,how='left')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multipliation matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "################################### Multipliation matrix #####################################\n",
    "##############################################################################################\n",
    "\n",
    "def settingMatrix( matrix_user,product_info):\n",
    "    # Setting the product matrix with only the necessarly data\n",
    "    del matrix_user['age']\n",
    "    del matrix_user['sexe']\n",
    "    del matrix_user['situation']\n",
    "    del matrix_user['zone']\n",
    "    cols = [col for col in matrix_user.columns if col not in ['user_id']]\n",
    "    matrix_product=product_info[cols]\n",
    "    matrix_product.index=product_info['id']\n",
    "    #matrix_product.to_csv('test_export_matrix_product.csv',sep=';')\n",
    "    #product_info.to_csv('test_export_product_info.csv',sep=';',encoding='utf-8')\n",
    "    return matrix_product\n",
    "\n",
    "def multiData(matrix_user,matrix_product):\n",
    "    # Multiply the data\n",
    "    return pd.DataFrame(matrix_product.values*matrix_user.values, columns=matrix_product.columns, index=matrix_product.index)\n",
    "\n",
    "def scoring( matrix_result_temp):\n",
    "    # Summing by columns\n",
    "    return matrix_result_temp.sum(axis=1).to_frame('score_analytics')\n",
    "\n",
    "def sortingData(product_info,matrix_result,asHist,dict_pond_mod):\n",
    "    # Transforming into dataframe for sorting\n",
    "    product_info.index=product_info['id']\n",
    "    # Concate simil and user form\n",
    "    if ~asHist:\n",
    "        product_info['hist_user_dist']=0\n",
    "\n",
    "    result_final=pd.merge(product_info,matrix_result, left_on=product_info.index.values,right_on=matrix_result.index.values,how='left')  \n",
    "    # Combining recommendation systems\n",
    "    #print result_final['score_analytics'].unique()\n",
    "    #print result_final['hist_user_dist'].unique()\n",
    "    # Suppress price=0\n",
    "    result_final=result_final[result_final['prix']>0]\n",
    "    #print result_final['dist_req'].unique()\n",
    "    result_final['order_reco']=result_final['score_analytics']*dict_pond_mod['form_user']+result_final['hist_user_dist']*dict_pond_mod['hist_user']+result_final['dist_req']*dict_pond_mod['req_sim']\n",
    "    return result_final.groupby(['walking','magId','requete']).apply(lambda x:x.sort_values(by=['walking','order_reco','position','price_unit'], ascending=[True,False,True,True]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping result to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "################################# Mapping result to json #####################################\n",
    "##############################################################################################\n",
    "    \n",
    "# Reverse mapping : DataFrame to json\n",
    "def mapToJson(select_final):\n",
    "    \"\"\"Conversion du dataframe de resultats en json pour etre envoye a apps\n",
    "\n",
    "    Args:\n",
    "        result: resultats avec le score des produits pour lutilisateur\n",
    "\n",
    "    Returns:\n",
    "        json de resultat\n",
    "    \"\"\"\n",
    "    res=collections.OrderedDict()\n",
    "    for ids,id_mag in enumerate(select_final.magId.unique()):\n",
    "        #print id_mag\n",
    "        #data={}\n",
    "        df_req=select_final[select_final['magId']==id_mag]\n",
    "        firsts=df_req.groupby('requete').first()\n",
    "        data2={}\n",
    "        for i in range(len(firsts)):\n",
    "            #print 'i', i\n",
    "            #data2[firsts.index[i]] = firsts['nomProduit'].values[i]\n",
    "            data2[firsts.index[i]] = firsts['id'].values[i]\n",
    "        res[id_mag]=data2\n",
    "        #res.append(data)\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "################################# Saving the results #########################################\n",
    "##############################################################################################\n",
    "\n",
    "def saveResults(features_df,result_final):\n",
    "    \"\"\" Save the recommandation into a database \"\"\"\n",
    "    # Shaping the dataframe\n",
    "    df_to_save=pd.DataFrame(result_final)\n",
    "    df_to_save['userId']=features_df['features']['idUser'][0]\n",
    "    df_to_save['date']=time()\n",
    "    df_to_save['clipped']=''\n",
    "    df_to_save['burned']=''\n",
    "    del df_to_save['analytics_category_list']\n",
    "    del df_to_save['key_0']\n",
    "    df_to_save.rename(columns={'id': 'idProd'}, inplace=True)\n",
    "\n",
    "    # First way : dataiku\n",
    "    #time_dtk=time()\n",
    "    #output_ds = dtk.Dataset('userListeCourse_prepared',project_key='BONSPLANS')\n",
    "    #output_ds.write_from_dataframe(df_to_save)\n",
    "    #print 'Time Saving Dataiku :',time()-time_dtk\n",
    "\n",
    "    # Second way : sql\n",
    "\n",
    "    # Writing the data\n",
    "    del df_to_save['magCat']\n",
    "    del df_to_save['magHor']\n",
    "\n",
    "    time_msql=time()\n",
    "    # Connecting to DB\n",
    "    #time_connect_db=time()\n",
    "    engine = create_engine('mysql+mysqlconnector://dkuadmin:Dataiku!@localhost:3306/dataiku', echo=False)\n",
    "    #print 'Time to connect to DB :', time()-time_connect_db\n",
    "\n",
    "    #sql.write_frame(df_to_save,name='userlistecourse',con=conn, if_exists='append', flavor='mysql',index=False)\n",
    "    df_to_save.to_sql(name='userlistecourse',con=engine, if_exists='append',index=False)\n",
    "    conn.close()\n",
    "    #req='INSERT INTO userlistecourse \n",
    "    #pd.read_sql(req, conn)\n",
    "\n",
    "\n",
    "    #print 'Time mysql :',time()-time_msql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "##################################### Prediction ou calcul ###################################\n",
    "##############################################################################################          \n",
    "        \n",
    "def predict( features_df):\n",
    "\n",
    "    # Note: this sample uses the second form (decision_series, proba_df)\n",
    "\n",
    "    # Note: this sample \"cheats\" and always returns 5 predictions.\n",
    "    # You should actually return 1 prediction per row in the features_df\n",
    "\n",
    "    ##### Liste des variables #####\n",
    "    #list_prod=['pain','oeuf','champagne','eau']\n",
    "    list_cat={'1000':'Alimentation','1001':'Puericulture et Enfants','1002':'Equipements de la maison et High-Tech',\n",
    "      '1003':'Sports, Loisirs et Culture','1004':'Bricolage, Decoration, Jardinerie et Animalerie',\n",
    "      '1005':'Mode et Accessoires','1006':'Auto et Moto','1007':'Beaute Sante et Bien-etre',\n",
    "      '1008':'Hotel Restaurant et Cafes','1009':'Banques et Assurances','1010':'Voyages et transports',\n",
    "      '1011':'Services',\n",
    "      '2000':'Bebe','2001':'Boissons','2002':'Boucherie','2003':'Boulangerie','2004':'Charcuterie et Traiteur',\n",
    "      '2005':'Cremerie','2006':'Epicerie salee','2007':'Epicerie sucree','2008':'Fruits frais','2009':'Legumes frais',\n",
    "      '2010':'Produits de la mer','2011':'Produits dietetiques','2012':'Surgeles'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #print \"Features DataFrame %s\" % features_df\n",
    "\n",
    "    # Get ponderation models\n",
    "    dict_pond_mod=getDictPond()\n",
    "\n",
    "    # Get raw data\n",
    "    get_raw_time=time()\n",
    "    rawJsonLC=mapFeatureDF(features_df)\n",
    "    get_r_d=time()-get_raw_time\n",
    "    print 'Temps recuperation raw data : ',get_r_d\n",
    "\n",
    "    # Get initial product infos\n",
    "    get_coupon_time=time()\n",
    "    product_info_raw=getCouponsData(rawJsonLC)\n",
    "    if len(product_info_raw.index)>0:\n",
    "        product_info=joinBPMag(rawJsonLC,product_info_raw)\n",
    "        get_i=time()-get_coupon_time\n",
    "        print 'Temps recuperation coupons : ',get_i\n",
    "\n",
    "        # List columns to add into product_info\n",
    "        time_mat_prod=time()\n",
    "        list_user=['universConso','bioSens','prixSens','reductionSens']\n",
    "        list_index=matrixProductColumnsAndMajProducts(product_info,list_user,list_cat)\n",
    "\n",
    "        # Creation of matrix product\n",
    "        matrix_product=matrixProductInit(list_index)\n",
    "        get_p=time()-time_mat_prod\n",
    "        print 'Temps creation matrix product empty', get_p\n",
    "\n",
    "        # Get user  infos\n",
    "        get_user_info_time=time()\n",
    "        loc_user=locUser(features_df)\n",
    "        user_id=getUserId(features_df)\n",
    "        matrix_user=getUsersMatrix(user_id)\n",
    "        mode_user=modeUser(matrix_user)\n",
    "        get_us=time()-get_user_info_time\n",
    "        print 'Temps recuperation infos user : ',get_us\n",
    "\n",
    "        # Enrich product infos\n",
    "        get_product_infos=time()\n",
    "        product_info=enrichDataProduct(product_info,loc_user, mode_user)\n",
    "        #product_info.to_csv('n_enrich_product.csv',sep=';')\n",
    "        get_produ=time()-get_product_infos\n",
    "        print 'Temps enrichissement produit: ',get_produ\n",
    "\n",
    "        # Mutlipy the matrix\n",
    "        multi_matrix_time=time()\n",
    "        matrix_product=settingMatrix(matrix_user,product_info)\n",
    "        del matrix_user['user_id']\n",
    "        matrix_result_temp=multiData(matrix_user,matrix_product)\n",
    "        matrix_result=scoring(matrix_result_temp)\n",
    "        #matrix_result.to_csv('n_result.csv',sep=';')\n",
    "        get_mutl=time()-multi_matrix_time\n",
    "        print 'Temps multiplication matrice: ',get_mutl\n",
    "\n",
    "        # Get historic buyings\n",
    "        time_simil=time()\n",
    "        # Get bought bons plans\n",
    "        old_plans=getHistoricBuy(features_df)\n",
    "        asH=asHist(old_plans)\n",
    "        if asH:\n",
    "            old_new_bp=mergeNewOldDB(product_info,features_df,old_plans)\n",
    "            res=similarityCalculation(old_new_bp)\n",
    "            res_scale=convertNeighborUnit(res)\n",
    "            result=getHistoricalWeight(old_new_bp, res_scale)\n",
    "            product_info=mergeScoringToListeCourse(product_info,result)\n",
    "            del product_info['weight']\n",
    "        time_similarity=time()-time_simil\n",
    "        print 'Time calculate similarity : ', time_similarity\n",
    "\n",
    "        # Request similarity\n",
    "        time_reqsim=time()\n",
    "        product_info=addReqDistCol(product_info)\n",
    "        time_reqsimil=time()-time_reqsim\n",
    "        print 'Time Request Similarity : ', time_reqsimil\n",
    "\n",
    "        # Sort data\n",
    "        sort_data_time=time()\n",
    "        result_final=sortingData(product_info,matrix_result,asH,dict_pond_mod)\n",
    "        #result_final.to_csv('test_export_result_final.csv',sep=';',encoding='utf-8')\n",
    "        #result_final.to_csv('n_result_final.csv',sep=';',encoding='utf-8',decimal=',')\n",
    "        sort_time=time()-sort_data_time\n",
    "        print 'Temps sorting data: ',sort_time\n",
    "\n",
    "        # Mapping the result\n",
    "        time_map_result=time()\n",
    "        res=mapToJson(result_final)\n",
    "        json_data=json.dumps(res,indent=3)\n",
    "        time_map_js=time()-time_map_result\n",
    "        print 'map results: ',time_map_js\n",
    "\n",
    "        # Saving the results\n",
    "        time_resultsave=time()\n",
    "        #saveResults(features_df,result_final)\n",
    "        save_t=time()-time_resultsave\n",
    "        print 'save results: ',save_t\n",
    "\n",
    "        # predictions, one per record (features_df row)\n",
    "        predictions = pd.Series(json_data)\n",
    "\n",
    "        # Printing execution time\n",
    "        print 100*'*'\n",
    "        print 'Temps recuperation raw data : ',get_r_d\n",
    "        print 'Temps recuperation coupons : ',get_i\n",
    "        print 'Temps creation matrix product empty', get_p\n",
    "        print 'Temps recuperation infos user : ',get_us\n",
    "        print 'Temps enrichissement produit: ',get_produ\n",
    "        print 'Temps multiplication matrice: ',get_mutl\n",
    "        print 'Temps calculate similarity : ', time_similarity\n",
    "        print 'Temps request similarity : ', time_reqsimil\n",
    "        print 'Temps sorting data: ',sort_time\n",
    "        print 'Temps map results: ',time_map_js\n",
    "        print 'Temps save results: ',save_t\n",
    "        print 50*'*'\n",
    "        print 'Total time: ',time()-get_raw_time\n",
    "        print 100*'*'\n",
    "    else :\n",
    "        predictions = pd.Series([json.dumps([''],indent=3)])\n",
    "\n",
    "    return (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps recuperation raw data :  6.91413879395e-06\n",
      "RangeIndex(start=0, stop=124, step=1)\n",
      "Temps recuperation coupons :  0.0541110038757\n",
      "Temps creation matrix product empty 0.080983877182\n",
      "Time mysql : 0.276537895203\n",
      "Temps recuperation infos user :  0.276617050171\n",
      "PREDICT PICKLE 0.0174038410187\n",
      "Temps enrichissement produit:  0.40919303894\n",
      "Temps multiplication matrice:  0.00867199897766\n",
      "Time calculate similarity :  0.858129024506\n",
      "Time Request Similarity :  0.154882907867\n",
      "Temps sorting data:  0.0323619842529\n",
      "map results:  0.00736594200134\n",
      "save results:  9.53674316406e-07\n",
      "****************************************************************************************************\n",
      "Temps recuperation raw data :  6.91413879395e-06\n",
      "Temps recuperation coupons :  0.0541110038757\n",
      "Temps creation matrix product empty 0.080983877182\n",
      "Temps recuperation infos user :  0.276617050171\n",
      "Temps enrichissement produit:  0.40919303894\n",
      "Temps multiplication matrice:  0.00867199897766\n",
      "Temps calculate similarity :  0.858129024506\n",
      "Temps request similarity :  0.154882907867\n",
      "Temps sorting data:  0.0323619842529\n",
      "Temps map results:  0.00736594200134\n",
      "Temps save results:  9.53674316406e-07\n",
      "**************************************************\n",
      "Total time:  1.88640284538\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    {\\n   \"aHR0cHM6Ly93d3cubW9ub3ByaXguZnIvbW9ub3A...\n",
       "dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df = json.loads(open('../inputs_bouchons/liste_course.json').read())\n",
    "predict(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
